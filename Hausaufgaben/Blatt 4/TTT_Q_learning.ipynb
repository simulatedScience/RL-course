{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit40039fc9f5864e99b33450205d4ec4f3",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2: Q-Learning Tic Tac Toe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir initialisieren Q(s,a) für alle möglichen Zustands- Aktions-Paare mit 0. Anfangs sind also alle möglichen Aktionen gleich wahrscheinlich.\n",
    "\n",
    "Zur Optimierung der Laufzeit berechnen wir nicht in jedem Zustand neu alle möglichen Aktionen, sondern speichern für alle bereits getroffenen Zustände die dort möglichen Aktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementieren der Trainingsumgebung\n",
    "Wir initialisieren ein Dictionary `Q_table`, in dem die bisher probierten Zustands- Aktions- Paare mit einer Wertung gespeichert werden.\n",
    "\n",
    "Schlüssel sind dazu Tupel (Zustand, Aktion), wobei die Aktion eine Zahl von 0 bis 8 und Zustand ein Tupel mit 9 Elementen aus {0,1,2} sind.\n",
    "\n",
    "Außerdem initialisieren wir `action_dict` um alle in einem Zustand (als Schlüssel gegeben) möglichen Aktionen zu speichern.\n",
    "\n",
    "Die `eploration_rate` in der $n$-ten Episode ist $\\varepsilon^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(learning_rate, discount_factor, base_exploration_rate, num_episodes=1e4, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}):\n",
    "    \"\"\"\n",
    "    play Tic Tac Toe [num_episodes] times to learn using Q-Learning with the given learning rate and discount_factor.\n",
    "    inputs:\n",
    "        learning_rate - (float) in range [0,1] - alpha\n",
    "        discount_factor - (float) in range [0,1] - gamma\n",
    "        base_exploration_rate - (float) - the starting exploration rate\n",
    "        num_episodes - (int) - number of episodes for training\n",
    "        reward_dict - (dict) - a dictionary specifying the rewards for winning, losing and draw\n",
    "            -> must have keys \"win\", \"loss\", \"draw\"\n",
    "    returns:\n",
    "        (dict) - the Q-table after training with the given parameters\n",
    "    \"\"\"\n",
    "    Q_table = dict()    # assign values to every visited state-action pair\n",
    "    N_table = dict()    # counting how often each state-action pair was visited\n",
    "    action_dict = dict()    # save the possible actions for each state\n",
    "\n",
    "    games = []\n",
    "    exploration_rate = base_exploration_rate\n",
    "    for n in range(num_episodes):\n",
    "        # play episode\n",
    "        state_hist = play_episode(Q_table, action_dict, exploration_rate, discount_factor, learning_rate, reward_dict, N_table)\n",
    "        exploration_rate *= base_exploration_rate\n",
    "        games.append(state_hist)\n",
    "\n",
    "    print(\"final exploration rate:\", exploration_rate)\n",
    "    \n",
    "    return Q_table, N_table, games\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spielen einer Episode\n",
    "\n",
    "Während dem Training spielt der Algorithmus gegen sich selbst und erforscht so gleichzeitig die Zustände, wenn er selbst anfängt und wenn der Gegner anfängt.\n",
    "\n",
    "Durch festlegen des Zeichens des Startspielers ist in jedem Zustand eindeutig, welches Zeichen als nächstes gesetzt werden muss.\n",
    "\n",
    "Zur späteren Aktualisierung der Q-Werte werden alle Zustände und gewählten Aktionen gespeichert (in den Listen `state_history` und `action_history`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(Q_table, action_dict, exploration_rate, discount_factor=0.95, learning_rate=0.1, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}, N_table=dict()):\n",
    "    \"\"\"\n",
    "    self-play an entire episode\n",
    "    returns:\n",
    "        (list) - state history\n",
    "        (list) - action history\n",
    "    \n",
    "    action_dict is changed in-place\n",
    "    \"\"\" \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    while True:\n",
    "        state = tuple(field)\n",
    "        state_history.append(state)\n",
    "        # get possible actions\n",
    "        try:\n",
    "            actions = action_dict[state]\n",
    "        except KeyError:\n",
    "            actions = get_actions(field)\n",
    "            action_dict[state] = actions\n",
    "\n",
    "        if len(state_history) > 2: \n",
    "            # we know the state that resulted from the last action\n",
    "            update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict, N_table)\n",
    "\n",
    "        if len(actions) == 0:\n",
    "            break # game has ended\n",
    "\n",
    "        action = choose_action(state, actions, Q_table, exploration_rate=exploration_rate)\n",
    "        action_history.append(action)\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1 # toggle sign between 1 and 2\n",
    "\n",
    "    last_state = state_history[-2]\n",
    "    last_action = action_history[-1]\n",
    "    if not (last_state, last_action) in Q_table.keys():\n",
    "        Q_table[(last_state, last_action)] = 0\n",
    "        N_table[(last_state, last_action)] = 0\n",
    "    # print(\"before\", Q_table[(last_state, last_action)])\n",
    "    Q_table[(last_state, last_action)] += learning_rate*(reward_dict[\"win\"] - Q_table[(last_state, last_action)])\n",
    "    N_table[(last_state, last_action)] += 1\n",
    "    \n",
    "    return state_history\n",
    "\n",
    "\n",
    "def update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict, N_table):\n",
    "    \"\"\"\n",
    "    update the second to last state in the Q-table\n",
    "    returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    prev_state = state_history[-3] # S = state\n",
    "    prev_action = action_history[-2] # A = action\n",
    "    state = state_history[-1] # S' = next state after action A\n",
    "\n",
    "    reward = get_reward(list(state), actions, reward_dict) # R = Reward\n",
    "    next_rewards = [] # Q(S', a') for all actions a'\n",
    "    for action in actions:\n",
    "        try:\n",
    "            next_rewards.append(Q_table[(state, action)])\n",
    "        except KeyError:\n",
    "            Q_table[(state, action)] = 0\n",
    "            N_table[(state, action)] = 0\n",
    "            next_rewards.append(0)\n",
    "\n",
    "    if not (prev_state, prev_action) in Q_table.keys():\n",
    "        Q_table[(prev_state, prev_action)] = 0\n",
    "        N_table[(prev_state, prev_action)] = 0\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        test_value = str(Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "    # Q(S,A) += alpha*(R + gamma * max(S', a') - Q(S,A))\n",
    "    Q_table[(prev_state, prev_action)] += learning_rate*(reward + discount_factor * max(next_rewards, default=0) - Q_table[(prev_state, prev_action)])\n",
    "    N_table[(prev_state, prev_action)] += 1\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        if test_value != str(Q_table[((1,2,0,0,1,2,0,0,0),8)]):\n",
    "            print(\"old value:\", test_value)\n",
    "            print(\"reward was\", reward)\n",
    "            print(\"new value:\", Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "\n",
    "\n",
    "def get_reward(field, actions, reward_dict):\n",
    "    \"\"\"\n",
    "    return the reward for the given field and possible actions\n",
    "    \"\"\"\n",
    "    if len(actions) > 0:\n",
    "        reward = reward_dict[\"move\"]\n",
    "    else:\n",
    "        winner = game_ended(field, get_winner=True)\n",
    "        if winner == 0: #draw\n",
    "            reward = reward_dict[\"draw\"]\n",
    "        else:\n",
    "            reward = reward_dict[\"loss\"]\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wählen einer Aktion\n",
    "Zum Wählen der Aktion wird eine $\\varepsilon$-greedy Strategie genutzt.\n",
    "\n",
    "Wird damit ausgewählt bekanntes Wissen zu nutzen (Exploitation), so wird, falls es mehrere Aktionen mit maximalem Wert gibt, eine zufällige davon ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_action(state, actions, Q_table, exploration_rate=0):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the current Q-table and the current exploration rate\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # print(\"exploit\", r, exploration_rate)\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            try:\n",
    "                action_values.append(Q_table[(state,action)])\n",
    "            except KeyError:\n",
    "                action_values.append(0)\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwenden des Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit all diesen Funktionen können wir nun endlich den Algorithmus testen. Es epfiehlt sich den Agent mindestens über 10.000 Spiele zu trainieren, dies sollte nur wenige Sekunden dauern. Die Trainingszeit steigt wie zu erwarten ziemlich linear mit der Anzahl an Trainingsspielen.\n",
    "\n",
    "Im Folgenden wird die exploration_rate mit `1-(5/num_episodes)` so gewählt, dass sie am Ende des Trainings nahe 0 kommt, aber nicht zu lange so klein ist.\n",
    "\n",
    "Jeder nicht-terminal Zug wird mit einem kleinen Wert bestraft um schnelles gewinnen zu fördern.\n",
    "\n",
    "Es ist schwierig eine optimale Lernrate $\\alpha$ oder einen besonders guten discount_factor $\\gamma$ zu finden, da das Evaluieren, wie gut die KI nach dem Training noch nicht automatisiert ist.\n",
    "\n",
    "Mit mehr Zeit wäre es denkbar genetische Algorithmen für diese Werte anzuwenden und die trainierten Agenten gegeneinander spielen zu lassen. So könnte man bessere Startwerte finen.\n",
    "Dabei würden relativ kurze Trainingszeiten verwendet werden um dann mit den gefundenen, optimierten Startwerten hoffentlich einen besseren Agenten trainieren zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 0.00673676792504109\nWall time: 15 s\n16164\n16164\n"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "discount_factor = 0.95\n",
    "num_episodes = int(1e5)\n",
    "exploration_rate = 1-(5/num_episodes)\n",
    "reward_dict = {\"win\":2,      # reward for win\n",
    "               \"loss\":-1,    # reward for loss\n",
    "               \"draw\":0,     # reward for draw\n",
    "               \"move\":-0.05} # reward per non-terminal move\n",
    "\n",
    "%time Q_table, N_table, games = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)\n",
    "print(len(Q_table))\n",
    "print(len(N_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "average vists per state: 50.27140559267508\nmax visits: 81462 for state ((0, 0, 0, 0, 0, 0, 0, 0, 0), 2)\nmin visits: 0 for state ((0, 1, 2, 1, 0, 0, 2, 1, 2), 5)\n"
    }
   ],
   "source": [
    "def analyse_N_table(N_table):\n",
    "    n_games = len(N_table)\n",
    "    max_n = (0,float(\"-inf\"))\n",
    "    min_n = (0,float(\"inf\"))\n",
    "\n",
    "    n_sum = 0\n",
    "    for key,value in N_table.items():\n",
    "        if value < min_n[1]:\n",
    "            min_n = (key,value)\n",
    "        if value > max_n[1]:\n",
    "            max_n = (key,value)\n",
    "        n_sum += value\n",
    "    avg_n = n_sum/n_games\n",
    "    \n",
    "    print(f\"average vists per state: {avg_n}\")\n",
    "    print(f\"max visits: {max_n[1]} for state {max_n[0]}\")\n",
    "    print(f\"min visits: {min_n[1]} for state {min_n[0]}\")\n",
    "\n",
    "analyse_N_table(N_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Q_table(Q_table, filename=\"Q_table.txt\"):\n",
    "    \"\"\"\n",
    "    write the given Q-table into a file\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"Q_table = {\\n\")\n",
    "        for key, value in Q_table.items():\n",
    "            file.write(str(key) + \":\" + str(value) + \",\\n\")\n",
    "        file.write(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_Q_table(filename=\"Q_table.txt\"):\n",
    "    \"\"\"\n",
    "    import Q_table as dictionary:\n",
    "    keys are state-action pairs as a tuple of a tuple (9 integers: 0/1/2) and an integer (0-8)\n",
    "    values are the corresponding Q-values\n",
    "\n",
    "    Example:\n",
    "        Q_table[((0,0,0,0,1,0,0,0,0),2)] -> 0.3\n",
    "    \"\"\"\n",
    "    Q_table = dict()\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            if line == \"Q_table = {\\n\" or line == \"}\":\n",
    "                continue\n",
    "            state_action, value = line[:-2].split(\":\")\n",
    "            state = tuple([int(x.strip(\" \")) for x in state_action[2:-5].split(\",\")])\n",
    "            action = int(state_action[-2])\n",
    "            Q_table[(state, action)] = float(value)\n",
    "    return Q_table\n",
    "\n",
    "test = import_Q_table()\n",
    "export_Q_table(test, filename=\"Q_table2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_Q_table(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns einzelne Spiele aus dem Trainingsprozess ansehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n-------------\n|   |   |   |\n-------------\n|   | o |   |\n-------------\n|   |   |   |\n-------------\n-------------\n|   |   |   |\n-------------\n|   | o |   |\n-------------\n| x |   |   |\n-------------\n-------------\n|   | o |   |\n-------------\n|   | o |   |\n-------------\n| x |   |   |\n-------------\n-------------\n|   | o |   |\n-------------\n|   | o |   |\n-------------\n| x | x |   |\n-------------\n-------------\n|   | o |   |\n-------------\n|   | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o |   |\n-------------\n|   | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o |   |\n-------------\n| o | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o | x |\n-------------\n| o | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o | x |\n-------------\n| o | o | o |\n-------------\n| x | x | o |\n-------------\n#########################\n"
    }
   ],
   "source": [
    "def show_game(games, i):\n",
    "    for state in games[i]:\n",
    "        print_field(state)\n",
    "        # print(game_ended(list(state)))\n",
    "    print(\"#\"*25)\n",
    "# show_game(games, 0)\n",
    "show_game(games, num_episodes-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen durch Menschen\n",
    "\n",
    "Mit der folgenden Funktion können wir auch die KI testen indem wir selbst gegen sie spielen. Dabei werden die Züge ausschließlich nach exploitation gewählt, also immer ein optimaler Zug entsprechend den Q-Werten aus der tabelle `Q_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n-------------\n| o |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n-------------\n| o |   |   |\n-------------\n|   |   | x |\n-------------\n|   |   |   |\n-------------\n-------------\n| o |   |   |\n-------------\n|   | o | x |\n-------------\n|   |   |   |\n-------------\n-------------\n| o |   | x |\n-------------\n|   | o | x |\n-------------\n|   |   |   |\n-------------\n-------------\n| o |   | x |\n-------------\n|   | o | x |\n-------------\n|   |   | o |\n-------------\n'o' won!\n"
    }
   ],
   "source": [
    "def play_AI(Q_table):\n",
    "    start_player = \"\"\n",
    "    while not start_player in [\"me\", \"ai\"]:\n",
    "        start_player = input(\"Who starts? (me/ ai)\\n\")\n",
    "    \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    print_field(field)\n",
    "\n",
    "    playing = True\n",
    "    while playing:\n",
    "        actions = get_actions(field)\n",
    "        if start_player == \"ai\":\n",
    "            action = choose_action(tuple(field), actions, Q_table, exploration_rate=0)\n",
    "            # print(\"action had value:\", Q_table[(tuple(field), action)])\n",
    "        else:\n",
    "            action = -5\n",
    "            while not action in actions:\n",
    "                action = input(f\"Choose your action ({str(actions)[1:-1]})\\n\")\n",
    "                if action == \"end\":\n",
    "                    playing = False\n",
    "                    break\n",
    "                try:\n",
    "                    action = int(action)\n",
    "                except:\n",
    "                    pass\n",
    "        if not playing:\n",
    "            print(\"game ended\")\n",
    "            break\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1\n",
    "        print_field(field)\n",
    "        start_player = \"ai\" if start_player == \"me\" else \"me\"\n",
    "\n",
    "        winner = game_ended(field,get_winner=True)\n",
    "        if winner != None:\n",
    "            playing = False\n",
    "            if winner == 0:\n",
    "                print(\"draw!\")\n",
    "            elif winner == 1:\n",
    "                print(\"'o' won!\")\n",
    "            else:\n",
    "                print(\"'x' won!\")\n",
    "\n",
    "play_AI(Q_table_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selbst nach 1.000.000 Trainingsspielen kann man, mit etwas Glück, noch gegen den Algorithmus gewinnen.\n",
    "\n",
    "Die wenig (<=100 Spiele) trainierte KI ist sehr einfach zu schlagen. Dennoch scheint diese gegen die mehr trainierte KI zu gewinnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n| o | x |   |\n-------------\n| o | o |   |\n-------------\n| x | o | x |\n-------------\naction 2 has value 0.0\naction 5 has value 0.0\n"
    }
   ],
   "source": [
    "field = (1,2,0,1,1,0,2,1,2)\n",
    "print_field(field)\n",
    "for action in get_actions(list(field)):\n",
    "    print(f\"action {action} has value {Q_table[(field,action)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste zwei unterschiedliche KIs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ais(Q_table_1, Q_table_2, n_games=100, exploration_rate=0.1):\n",
    "    results = [0,0,0] # AI-1 victories, AI-2 victories, draws\n",
    "    for n in range(n_games):\n",
    "        field = [0]*9\n",
    "        active_index = n%2\n",
    "        Q_tables = (Q_table_1, Q_table_2)\n",
    "        sign = 1\n",
    "        winner = None\n",
    "        while winner==None:\n",
    "            actions = get_actions(field)\n",
    "            if actions == []:\n",
    "                print_field(field)\n",
    "            action = choose_action(tuple(field), actions, Q_tables[active_index], exploration_rate=exploration_rate)\n",
    "            field[action] = sign\n",
    "            sign = sign%2 + 1\n",
    "            winner = game_ended(field, get_winner=True)\n",
    "        if winner == 0:\n",
    "            results[2] += 1\n",
    "        elif n%2 == 0:\n",
    "            if winner == 1:\n",
    "                results[0] += 1\n",
    "            else:\n",
    "                results[1] += 1\n",
    "        elif n%2 == 1:\n",
    "            if winner == 1:\n",
    "                results[1] += 1\n",
    "            else:\n",
    "                results[0] += 1\n",
    "\n",
    "    print(\"KI 1 hat {} Mal gewonnen.\\nKI 2 hat {} Mal gewonnen.\\nEs gab {} Unentschieden.\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train AI 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 16.0\nWall time: 23.5 ms\naverage vists per state: 0.27586206896551724\nmax visits: 1 for state ((0, 1, 0, 2, 0, 0, 0, 0, 0), 2)\nmin visits: 0 for state ((0, 1, 0, 2, 0, 0, 0, 0, 0), 0)\n"
    }
   ],
   "source": [
    "# KI 1\n",
    "learning_rate = 0.05\n",
    "discount_factor = 0.80\n",
    "num_episodes = int(1e0)\n",
    "exploration_rate = 1-(5/num_episodes)\n",
    "reward_dict = {\"win\":1,      # reward for win\n",
    "               \"loss\":-2,    # reward for loss\n",
    "               \"draw\":0.5,     # reward for draw\n",
    "               \"move\":0} # reward per non-terminal move\n",
    "\n",
    "%time Q_table_1, N_table_1, _ = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)\n",
    "analyse_N_table(N_table_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train AI 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 0.049414393574685855\nWall time: 299 ms\naverage vists per state: 0.6824988516306845\nmax visits: 115 for state ((0, 0, 0, 0, 0, 0, 0, 0, 0), 6)\nmin visits: 0 for state ((1, 0, 0, 0, 0, 0, 2, 1, 0), 2)\n"
    }
   ],
   "source": [
    "# KI 2\n",
    "\n",
    "learning_rate = 0.05\n",
    "discount_factor = 0.75\n",
    "num_episodes = int(1e3)\n",
    "exploration_rate = 1-(3/num_episodes)\n",
    "reward_dict = {\"win\":1,      # reward for win\n",
    "               \"loss\":-1,    # reward for loss\n",
    "               \"draw\":0,     # reward for draw\n",
    "               \"move\":-0.05} # reward per non-terminal move\n",
    "\n",
    "%time Q_table_2, N_table_2, _ = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)\n",
    "analyse_N_table(N_table_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play AI 1 vs. AI 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "KI 1 hat 4550 Mal gewonnen.\nKI 2 hat 4192 Mal gewonnen.\nEs gab 1258 Unentschieden.\n"
    }
   ],
   "source": [
    "test_ais(Q_table_1, Q_table_2, n_games=10_000, exploration_rate=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch nach Ausprobieren zahlreicher Werte für $\\alpha$, $\\gamma$, $\\varepsilon$ und für verschiedene Rewards konnte ich keine Kombination finden, die die hier zweite KI konstant schlägt.\n",
    "\n",
    "Interessanterweise führt aber eine geringere Anzahl an Trainingsspielen zu mehr Siegen. Das zeigt deutlich, dass die KIs auch nach längerem Selbsttraining nicht perfekt sind. Eine genaue Erklärung für dieses Verhalten habe ich aber nicht gefunden.\n",
    "\n",
    "Die `exploration_rate` in den Testspielen hat kaum Einfluss auf das Sieg-Verhältnis, nur auf die gesamte Anzahl an Unentschieden."
   ]
  }
 ]
}