{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit40039fc9f5864e99b33450205d4ec4f3",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ziel\n",
    "Wir wollen mittels der Q-Matrix, die wir mit Q-Learning erzeugen ein neuronales Netz trainieren um eine KI zu erschaffen, die besser Tic Tac Toe spielen kann als die bisherige.\n",
    "\n",
    "# erste Aufgaben\n",
    "Wir wollen ein Neuronales Netz trainieren. Dafür müssen wir es zuerst erzeugen, wofür wir die Keras Bibliothek nutzen werden.\n",
    "\n",
    "Zum Trainieren müssen wir außerdem die Trainingsdaten in ein geeignetes Format bringen.\n",
    "\n",
    "Also verfolgen wir die folgenden Schritte:\n",
    "\n",
    "  1. Bereite die Trainingsdaten vor.\n",
    "  2. Erstelle das Neuronales Netz.\n",
    "  3. Trainiere das Netzwerk mit diesen Trainingsdaten.\n",
    "  4. Teste das Netzwerk.\n",
    "  5. Speichere das Netzwerk um es nicht jedes Mal neu trainieren zu müssen.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Traingsdaten vorbereiten\n",
    "\n",
    "Wir werden das neuronale Netz später mit der `.fit()` Methode von Keras trainieren. Dazu müssen die Daten folgendes Format haben:\n",
    "\n",
    "- ein array mit Inputwerten im Bereich $[0,1]$ pro Input Node\n",
    "- ein array mit Zielwerten (Outputwerten), ebenfalls im Bereich $[0,1]$\n",
    "\n",
    "## unsere bisherigen Daten\n",
    "Bisher sind unsere Trainingsdaten in einem Dictionary in folgender Form gespeichert\n",
    "\n",
    "**Keys:**\n",
    "- Paare (state, action), wobei\n",
    "\n",
    "    - state ein Tupel aus 9 Integern 0/1/2 ist.\n",
    "    - action ein einzelner Integer von 0 bis 8 ist.\n",
    "\n",
    "**Values:**\n",
    "- Floats, deren Bereich von den Trainingsdaten abhängt.\n",
    "\n",
    "    - minimaler Wert ist `reward_dict[\"loss\"]`\n",
    "    - maximaler Wert ist `reward_dict[\"win\"]`\n",
    "\n",
    "    ...zumindest wenn dies die Extremwerte der Rewards sind, was aber logischerweise der Fall sein sollte\n",
    "\n",
    "## was wir für Daten wollen\n",
    "Das Neuronale Netzwerk soll die Q-Funktion approximieren.\n",
    "Nun haben wir zwei Möglichkeiten, welche Funktion das Netzwerk approximieren soll:\n",
    "\n",
    "### #1 Eingabewerte nur Zustand\n",
    "$NN: \\mathcal{S} \\to \\mathbb{R}^{|\\mathcal{A}|}$ das heißt das Netzwerk bekommt ein Zustand als Eingabewert und gibt für alle Aktionen, die in dem Zustand möglich sind, eine Wertung aus.\n",
    "\n",
    "Die Anzahl der möglichen Aktionen ist aber abhängig vom Zustand. Das heißt wir bräuchten immer 9 Ausgabewerte (für jede Aktion einen), aber das Netzwerk müsste zusätzlich lernen, welche Aktionen erlaubt sind.\n",
    "\n",
    "Alternativ könnte man hier auch einen einzelnen Ausgabewert als die möglichen Aktionen interpretieren. Also durch eine Zustandsabhängige Funktion $f_S:[0,1] \\to A\\subset \\{0,...,8\\}$.\n",
    "\n",
    "### #2 Eingabewerte Zustands-Aktions Paare\n",
    "$NN: \\mathcal{S} \\to \\mathbb{R}$ Hier würde das Netzwerk zusätzlich zum Zustand des Feldes noch eine Aktion als Input bekommen.\n",
    "\n",
    "Der Ausgabewert ist dann eine einzige Zahl, die den Wert dieser Aktion repräsentiert.\n",
    "\n",
    "## Pro und Cons\n",
    "Variante 2 erfordert es für jede mögliche Aktion den Wert zu berechnen bevor ein Zug gewählt werden kann. Variante 1 verkürzt diesen Prozess indem einfach nur der Maximale Output-Wert genommen wird.\n",
    "\n",
    "Dieser Unterschied kann durch Wahl der Größe und Komplexität des Netzwerks ausgeglichen werden. Da das Netzwerk in Variante 2 nicht lernen muss, welche Züge überhaupt erlaubt sind, sollte das Training damit aber schneller gehen.\n",
    "\n",
    "Sicherlich gibt es noch viele andere Möglichkeiten der Implementierung, aber ich werde hier Variante 2 verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wahl von Variante 2 erlaubt außerdem einfaches Vorbereiten der Daten.\n",
    "\n",
    "Wir generieren die input Daten also durch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prepare_data(Q_table):\n",
    "    \"\"\"\n",
    "    prepare the data given in a Q-table for training the neural network\n",
    "    \"\"\"\n",
    "    training_inputs = []\n",
    "    training_outputs = []\n",
    "    for state_action, value in Q_table.items():\n",
    "        # input_data   =      state_info       +    action_info\n",
    "        training_inputs.append( list(state_action[0]) + [state_action[1]] )\n",
    "        training_outputs.append ( (value+1)/3 ) #make sure the target value is in range [0,1]\n",
    "    return np.array(training_inputs), np.array(training_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Erstelle das Netzwerk (`model`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_ttt_network():\n",
    "    model = keras.Sequential()\n",
    "    model.add( keras.Input(shape=(10,)) ) # input layer - 10 Nodes\n",
    "    model.add( layers.Dense(5) )   # hidden layer 1 - 10 Nodes\n",
    "    model.add( layers.Dense(5) )   # hidden layer 2 - 10 Nodes\n",
    "    model.add( layers.Dense(1) )   # output layer - 1 Node\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Trainiere das Netzwerk mit `.fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, samples, labels, epochs=50, batch_size=32):\n",
    "    return model.fit(samples, labels, epochs=epochs, batch_size=batch_size, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun nutzen wir die oben definierten Methoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_Q_table(filename=\"Q_table.txt\"):\n",
    "    \"\"\"\n",
    "    import Q_table as dictionary:\n",
    "    keys are state-action pairs as a tuple of a tuple (9 integers: 0/1/2) and an integer (0-8)\n",
    "    values are the corresponding Q-values\n",
    "\n",
    "    Example:\n",
    "        Q_table[((0,0,0,0,1,0,0,0,0),2)] -> 0.3\n",
    "    \"\"\"\n",
    "    Q_table = dict()\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            if line == \"Q_table = {\\n\" or line == \"}\":\n",
    "                continue\n",
    "            state_action, value = line[:-2].split(\":\")\n",
    "            state = tuple([int(x.strip(\" \")) for x in state_action[2:-5].split(\",\")])\n",
    "            action = int(state_action[-2])\n",
    "            Q_table[(state, action)] = float(value)\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(16164, 10)\nEpoch 1/10\n1617/1617 [==============================] - 1s 546us/step - loss: 0.0218\nEpoch 2/10\n1617/1617 [==============================] - 2s 939us/step - loss: 0.0039\nEpoch 3/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0039\nEpoch 4/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0039\nEpoch 5/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0039\nEpoch 6/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0039\nEpoch 7/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0039\nEpoch 8/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0038\nEpoch 9/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0038\nEpoch 10/10\n1617/1617 [==============================] - 2s 1ms/step - loss: 0.0038\n"
    }
   ],
   "source": [
    "# generate data\n",
    "Q_table = import_Q_table(filename=\"Q_table.txt\")\n",
    "# prepare data\n",
    "inputs, outputs = prepare_data(Q_table)\n",
    "print(inputs.shape)\n",
    "# create model\n",
    "model = create_ttt_network()\n",
    "# train network with given data\n",
    "history = train_network(model, inputs, outputs, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_23\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_52 (Dense)             (None, 5)                 55        \n_________________________________________________________________\ndense_53 (Dense)             (None, 5)                 30        \n_________________________________________________________________\ndense_54 (Dense)             (None, 1)                 6         \n=================================================================\nTotal params: 91\nTrainable params: 91\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiele mit der KI\n",
    "\n",
    "zuerst nutzen wir wieder zwei Methoden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_NN_action(state, actions, model, exploration_rate=0):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the given neural network (model) and the current exploration rate\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            action_values.append( model.predict( [list(state)+[action]])[0][0] )\n",
    "        print(list(zip(actions, action_values)))\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(2, 0.35656887), (3, 0.35582227), (6, 0.35358268), (7, 0.3528362), (8, 0.35208964)]\nchosen action: 2\n"
    }
   ],
   "source": [
    "state = [1,2,0,0,2,1,0,0,0]\n",
    "actions = get_actions(state)\n",
    "action = choose_NN_action(state, actions, model)\n",
    "print(f\"chosen action: {action}\")"
   ]
  }
 ]
}