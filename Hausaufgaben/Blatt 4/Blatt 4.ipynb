{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit40039fc9f5864e99b33450205d4ec4f3",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhaltsverzeichnis\n",
    "Die allgemeinen Tic Tac Toe Methoden sind in der beiliegenden Datei `TTT_functions.py` geschrieben um dieses Notebook etwas zu kürzen.\n",
    "### 1. Q-Learning Implementierung wie für Blatt 3\n",
    "\n",
    "### 2. Neuronales Netz\n",
    "\n",
    "### 3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTT_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Der Gegenspieler wird immer als Umwelt interpretiert. Das heißt wird von einem Zustand $S$ aus eine Aktion $A$ gewählt, so ist der Folgezustand $S'$ dadurch noch nicht eindeutig bestimmt. Dieser wird erst durch die nächste Aktion des Gegners festgelegt.\n",
    "\n",
    "So kann auch während dem Spiel gelernt werden, da die Wertung vom Zustands-Aktionspaar $(S,A)$ von der Übergangswahrscheinlichkeit $S \\to S'$ abhängt.\n",
    "\n",
    "Indem dies für beide Spieler gleichzeitig gemacht wird kann ein Spiel also die Q-Matrix mit Informationen über alle im Spiel vorkommenden Zustands-Aktionspaare aktualisiert werden, sodass die KI gleichzeitig das Verhalten als beginnender und als zweiter Spieler lernt.\n",
    "\n",
    "## Trainingsalgorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(learning_rate, discount_factor, base_exploration_rate, num_episodes=1e4, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}):\n",
    "    \"\"\"\n",
    "    play Tic Tac Toe [num_episodes] times to learn using Q-Learning with the given learning rate and discount_factor.\n",
    "    inputs:\n",
    "        learning_rate - (float) in range [0,1] - alpha\n",
    "        discount_factor - (float) in range [0,1] - gamma\n",
    "        base_exploration_rate - (float) - the starting exploration rate\n",
    "        num_episodes - (int) - number of episodes for training\n",
    "        reward_dict - (dict) - a dictionary specifying the rewards for winning, losing and draw\n",
    "            -> must have keys \"win\", \"loss\", \"draw\"\n",
    "    returns:\n",
    "        (dict) - the Q-table after training with the given parameters\n",
    "    \"\"\"\n",
    "    Q_table = dict()    # assign values to every visited state-action pair\n",
    "    N_table = dict()    # counting how often each state-action pair was visited\n",
    "    action_dict = dict()    # save the possible actions for each state\n",
    "\n",
    "    games = []\n",
    "    exploration_rate = base_exploration_rate\n",
    "    for n in range(num_episodes):\n",
    "        # play episode\n",
    "        state_hist = play_episode(Q_table, action_dict, exploration_rate, discount_factor, learning_rate, reward_dict, N_table)\n",
    "        exploration_rate *= base_exploration_rate\n",
    "        games.append(state_hist)\n",
    "\n",
    "    print(\"final exploration rate:\", exploration_rate)\n",
    "    \n",
    "    return Q_table, N_table, games\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulieren einer Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(Q_table, action_dict, exploration_rate, discount_factor=0.95, learning_rate=0.1, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}, N_table=dict()):\n",
    "    \"\"\"\n",
    "    self-play an entire episode\n",
    "    returns:\n",
    "        (list) - state history\n",
    "        (list) - action history\n",
    "    \n",
    "    action_dict is changed in-place\n",
    "    \"\"\" \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    while True:\n",
    "        state = tuple(field)\n",
    "        state_history.append(state)\n",
    "        # get possible actions\n",
    "        try:\n",
    "            actions = action_dict[state]\n",
    "        except KeyError:\n",
    "            actions = get_actions(field)\n",
    "            action_dict[state] = actions\n",
    "\n",
    "        if len(state_history) > 2: \n",
    "            # we know the state that resulted from the last action\n",
    "            update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict, N_table)\n",
    "\n",
    "        if len(actions) == 0:\n",
    "            break # game has ended\n",
    "\n",
    "        action = choose_Q_action(state, actions, Q_table, exploration_rate=exploration_rate)\n",
    "        action_history.append(action)\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1 # toggle sign between 1 and 2\n",
    "\n",
    "    last_state = state_history[-2]\n",
    "    last_action = action_history[-1]\n",
    "    if not (last_state, last_action) in Q_table.keys():\n",
    "        Q_table[(last_state, last_action)] = 0\n",
    "        N_table[(last_state, last_action)] = 0\n",
    "    # print(\"before\", Q_table[(last_state, last_action)])\n",
    "    Q_table[(last_state, last_action)] += learning_rate*(reward_dict[\"win\"] - Q_table[(last_state, last_action)])\n",
    "    N_table[(last_state, last_action)] += 1\n",
    "    \n",
    "    return state_history\n",
    "\n",
    "\n",
    "def update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict, N_table):\n",
    "    \"\"\"\n",
    "    update the second to last state in the Q-table\n",
    "    returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    prev_state = state_history[-3] # S = state\n",
    "    prev_action = action_history[-2] # A = action\n",
    "    state = state_history[-1] # S' = next state after action A\n",
    "\n",
    "    reward = get_reward(list(state), actions, reward_dict) # R = Reward\n",
    "    next_rewards = [] # Q(S', a') for all actions a'\n",
    "    for action in actions:\n",
    "        try:\n",
    "            next_rewards.append(Q_table[(state, action)])\n",
    "        except KeyError:\n",
    "            Q_table[(state, action)] = 0\n",
    "            N_table[(state, action)] = 0\n",
    "            next_rewards.append(0)\n",
    "\n",
    "    if not (prev_state, prev_action) in Q_table.keys():\n",
    "        Q_table[(prev_state, prev_action)] = 0\n",
    "        N_table[(prev_state, prev_action)] = 0\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        test_value = str(Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "    # Q(S,A) += alpha*(R + gamma * max(S', a') - Q(S,A))\n",
    "    Q_table[(prev_state, prev_action)] += learning_rate*(reward + discount_factor * max(next_rewards, default=0) - Q_table[(prev_state, prev_action)])\n",
    "    N_table[(prev_state, prev_action)] += 1\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        if test_value != str(Q_table[((1,2,0,0,1,2,0,0,0),8)]):\n",
    "            print(\"old value:\", test_value)\n",
    "            print(\"reward was\", reward)\n",
    "            print(\"new value:\", Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "\n",
    "\n",
    "def get_reward(field, actions, reward_dict):\n",
    "    \"\"\"\n",
    "    return the reward for the given field and possible actions\n",
    "    \"\"\"\n",
    "    if len(actions) > 0:\n",
    "        reward = reward_dict[\"move\"]\n",
    "    else:\n",
    "        winner = game_ended(field, get_winner=True)\n",
    "        if winner == 0: #draw\n",
    "            reward = reward_dict[\"draw\"]\n",
    "        else:\n",
    "            reward = reward_dict[\"loss\"]\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswählen einer Aktion\n",
    "Die Aktionen werden nach einer $\\varepsilon$-greedy Strategie ausgewählt. $\\varepsilon$ ist dabei die Wahrscheinlichkeit, dass Exploration, also eine zufällige Aktion gewählt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_Q_action(state, actions, Q_table, exploration_rate=0):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the current Q-table and the current exploration rate\n",
    "    inputs:\n",
    "    -------\n",
    "        state - (tuple) or (list) - the state as a tuple or list\n",
    "        actions (tuple) or (list) - all possible actions in the given state\n",
    "        Q_table - (dict) - dictionary storing all known Q-values\n",
    "        exploration_rate - (float) in [0,1] - probability of choosing exploration rather than exploitation\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # print(\"exploit\", r, exploration_rate)\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            try:\n",
    "                action_values.append(Q_table[(state,action)])\n",
    "            except KeyError:\n",
    "                action_values.append(0)\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwenden des Q-Learning Algorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 0.00673676792504109\nWall time: 7.66 s\n"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "discount_factor = 0.95\n",
    "num_episodes = int(1e5)\n",
    "exploration_rate = 1-(5/num_episodes)\n",
    "reward_dict = {\"win\":1,      # reward for win\n",
    "               \"loss\":-1,    # reward for loss\n",
    "               \"draw\":0,     # reward for draw\n",
    "               \"move\":-0.05} # reward per non-terminal move\n",
    "\n",
    "%time Q_table, N_table, games = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern der Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Q_table(Q_table, filename=\"Q_table.txt\"):\n",
    "    \"\"\"\n",
    "    write the given Q-table into a file\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"Q_table = {\\n\")\n",
    "        for key, value in Q_table.items():\n",
    "            file.write(str(key) + \":\" + str(value) + \",\\n\")\n",
    "        file.write(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_Q_table(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netz\n",
    "### Idee:\n",
    "Wir nutzen die zuvor erzeugt Q-Matrix um ein Neuronales Netzwerk zu trainieren, welches die Q-Funktion approximiert.\n",
    "\n",
    "Dazu wird das Netzwerk ein Zustands-Aktions Paar als Eingabe bekommen und eine Wertung ($\\in \\mathbb{R}$) zurückgeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsdaten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prepare_data(Q_table):\n",
    "    \"\"\"\n",
    "    prepare the data given in a Q-table for training the neural network\n",
    "    \"\"\"\n",
    "    training_inputs = []\n",
    "    training_outputs = []\n",
    "    for state_action, value in Q_table.items():\n",
    "        # input_data   =      state_info       +    action_info\n",
    "        training_inputs.append( list(state_action[0]) + [state_action[1]/4] )\n",
    "        training_outputs.append ( (value+1)/3 ) #make sure the target value is in range [0,1]\n",
    "    return np.array(training_inputs), np.array(training_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_ttt_network(hidden_layers):\n",
    "    model = keras.Sequential()\n",
    "    model.add( keras.Input(shape=(10,)) ) # input layer - 10 Nodes\n",
    "    for size in hidden_layers:\n",
    "        model.add( layers.Dense(size) )\n",
    "    model.add( layers.Dense(1) )   # output layer - 1 Node\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk trainieren\n",
    "\n",
    "Hier wird das Netzwerk mit den oben generierten Daten trainiert. Anstatt oben eine neue Q-Matrix zu berechnen kann auch eine bekannte aus einer Datei geladen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, samples, labels, epochs=50, batch_size=32):\n",
    "    return model.fit(samples, labels, epochs=epochs, batch_size=batch_size, use_multiprocessing=True)\n",
    "\n",
    "def import_Q_table(filename=\"Q_table.txt\"):\n",
    "    \"\"\"\n",
    "    import Q_table as dictionary:\n",
    "    keys are state-action pairs as a tuple of a tuple (9 integers: 0/1/2) and an integer (0-8)\n",
    "    values are the corresponding Q-values\n",
    "\n",
    "    Example:\n",
    "        Q_table[((0,0,0,0,1,0,0,0,0),2)] -> 0.3\n",
    "    \"\"\"\n",
    "    Q_table = dict()\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            if line == \"Q_table = {\\n\" or line == \"}\":\n",
    "                continue\n",
    "            state_action, value = line[:-2].split(\":\")\n",
    "            state = tuple([int(x.strip(\" \")) for x in state_action[2:-5].split(\",\")])\n",
    "            action = int(state_action[-2])\n",
    "            Q_table[(state, action)] = float(value)\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Q-table from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally generate data\n",
    "Q_table = import_Q_table(filename=\"Q_table.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actually train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.   0.   0.   0.   0.   1.   2.   0.   0.   0.  ]\n [0.   0.   0.   0.   0.   1.   2.   0.   0.   0.25]\n [0.   0.   0.   0.   0.   1.   2.   0.   0.   0.5 ]\n [0.   0.   0.   0.   0.   1.   2.   0.   0.   0.75]\n [0.   0.   0.   0.   0.   1.   2.   0.   0.   1.  ]\n [0.   0.   0.   0.   0.   1.   2.   0.   0.   1.75]\n [0.   0.   0.   0.   0.   1.   2.   0.   0.   2.  ]\n [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.25]\n [1.   0.   0.   0.   0.   1.   2.   0.   0.   0.25]\n [1.   0.   0.   0.   0.   1.   2.   0.   0.   0.5 ]]\n[0.33014686 0.32935545 0.33005992 0.33016879 0.33011688 0.32884704\n 0.33011004 0.31533826 0.33235608 0.33234883]\nusing 16164 test values\nEpoch 1/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.1009\nEpoch 2/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0034\nEpoch 3/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0034\nEpoch 4/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0035\nEpoch 5/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0036\nEpoch 6/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0036\nEpoch 7/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0037\nEpoch 8/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0036\nEpoch 9/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0036\nEpoch 10/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0036\nEpoch 11/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0036\nEpoch 12/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0035\nEpoch 13/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0035\nEpoch 14/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0035\nEpoch 15/15\n1078/1078 [==============================] - 1s 1ms/step - loss: 0.0035\n"
    }
   ],
   "source": [
    "# prepare data\n",
    "inputs, outputs = prepare_data(Q_table)\n",
    "print(f\"using {inputs.shape[0]} test values\")\n",
    "# create model\n",
    "model = create_ttt_network([10,5])\n",
    "# train network with given data\n",
    "history = train_network(model, inputs, outputs, epochs=15, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktion auswählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_NN_action(state, actions, model, exploration_rate=0):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the given neural network (model) and the current exploration rate\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            action_values.append( model.predict( [list(state)+[action/4]])[0][0] )\n",
    "        print(list(zip(actions, action_values)))\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spielen: Mensch vs. KI\n",
    "Bevor gespielt werden kann muss dies natürlich mit einigen funktionen vorbereitet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_AI(Q_table, network):\n",
    "    choose_action = get_ai_function(Q_table, network)\n",
    "    start_player = \"\"\n",
    "    while not start_player.lower() in [\"me\", \"ai\"]:\n",
    "        start_player = input(\"Who starts? (me/ ai)\\n\")\n",
    "    \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    print_field(field)\n",
    "\n",
    "    playing = True\n",
    "    while playing:\n",
    "        actions = get_actions(field)\n",
    "        if len(actions) == 0:\n",
    "            break\n",
    "        if start_player == \"ai\":\n",
    "            action = choose_action(tuple(field), actions, exploration_rate=0)\n",
    "        else:\n",
    "            action = get_human_action(actions)\n",
    "            if action == \"end\":\n",
    "                print(\"game interrupted\")\n",
    "                return\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1\n",
    "        print_field(field)\n",
    "        start_player = \"ai\" if start_player == \"me\" else \"me\"\n",
    "        playing = print_winner(field)\n",
    "\n",
    "\n",
    "def print_winner(field):\n",
    "        \"\"\"\n",
    "        if the given field is in a terminal state, print the winner.\n",
    "        \"\"\"\n",
    "        winner = game_ended(field, get_winner=True)\n",
    "        playing = True\n",
    "        if winner != None:\n",
    "            playing = False\n",
    "            if winner == 0:\n",
    "                print(\"draw!\")\n",
    "            elif winner == 1:\n",
    "                print(\"'o' won!\")\n",
    "            else:\n",
    "                print(\"'x' won!\")\n",
    "        return playing\n",
    "\n",
    "\n",
    "def get_human_action(actions):\n",
    "    \"\"\"\n",
    "    get user input for the next action\n",
    "    \"\"\"\n",
    "    action = -5\n",
    "    while not action in actions:\n",
    "        action = input(f\"Choose your action ({str(actions)[1:-1]})\\n\")\n",
    "        try:\n",
    "            action = int(action)\n",
    "            return action\n",
    "        except:\n",
    "            if action.lower() == \"end\":\n",
    "                return \"end\"\n",
    "\n",
    "\n",
    "def get_ai_function(Q_table, network):\n",
    "    \"\"\"\n",
    "    returns a function that chooses an AI-move based on a given field\n",
    "    inputs:\n",
    "        Q_table - (dict) - dictionary with Q_values\n",
    "        network - (keras.model) - keras model\n",
    "    returns:\n",
    "        (function) - a function that chooses an action.\n",
    "            arguments: (state, actions, exploration_rate=0)\n",
    "    \"\"\"\n",
    "    user_input = \"\"\n",
    "    while not user_input.upper() in [\"NN\", \"Q\"]:\n",
    "        user_input = input(\"What AI shall be the opponent?\\n(Neural Network -> NN or Q-Learning -> Q)\\n\")\n",
    "\n",
    "    if user_input.upper() == \"NN\":\n",
    "        def choose_ai_action(state, actions, exploration_rate=0):\n",
    "            return choose_NN_action(state, actions, network, exploration_rate=exploration_rate)\n",
    "    else:\n",
    "        def choose_ai_action(state, actions, exploration_rate=0):\n",
    "            return choose_Q_action(state, action, Q_table, exploration_rate=exploration_rate)\n",
    "    return choose_ai_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n[(0, 0.30547655), (1, 0.3070523), (2, 0.30862808), (3, 0.31020388), (4, 0.31177965), (5, 0.31335545), (6, 0.3149312), (7, 0.31650695), (8, 0.31808275)]\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   | o |\n-------------\n-------------\n|   |   |   |\n-------------\n|   | x |   |\n-------------\n|   |   | o |\n-------------\n[(0, 0.30680954), (1, 0.3083853), (2, 0.30996114), (3, 0.31153685), (5, 0.31468844), (6, 0.3162642), (7, 0.31784004)]\n-------------\n|   |   |   |\n-------------\n|   | x |   |\n-------------\n|   | o | o |\n-------------\n-------------\n|   |   |   |\n-------------\n|   | x |   |\n-------------\n| x | o | o |\n-------------\n[(0, 0.33672395), (1, 0.33829987), (2, 0.33987558), (3, 0.34145138), (5, 0.34460282)]\n-------------\n|   |   |   |\n-------------\n|   | x | o |\n-------------\n| x | o | o |\n-------------\n-------------\n|   |   | x |\n-------------\n|   | x | o |\n-------------\n| x | o | o |\n-------------\n'x' won!\n"
    }
   ],
   "source": [
    "play_AI(Q_table, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}