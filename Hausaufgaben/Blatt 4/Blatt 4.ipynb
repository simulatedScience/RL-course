{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit40039fc9f5864e99b33450205d4ec4f3",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhaltsverzeichnis\n",
    "Die allgemeinen Tic Tac Toe Methoden sind in der beiliegenden Datei `TTT_functions.py` geschrieben um dieses Notebook etwas zu kürzen.\n",
    "### 1. Q-Learning Implementierung wie für Blatt 3\n",
    "\n",
    "### 2. Neuronales Netz\n",
    "\n",
    "### 3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTT_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Der Gegenspieler wird immer als Umwelt interpretiert. Das heißt wird von einem Zustand $S$ aus eine Aktion $A$ gewählt, so ist der Folgezustand $S'$ dadurch noch nicht eindeutig bestimmt. Dieser wird erst durch die nächste Aktion des Gegners festgelegt.\n",
    "\n",
    "So kann auch während dem Spiel gelernt werden, da die Wertung vom Zustands-Aktionspaar $(S,A)$ von der Übergangswahrscheinlichkeit $S \\to S'$ abhängt.\n",
    "\n",
    "Indem dies für beide Spieler gleichzeitig gemacht wird kann ein Spiel also die Q-Matrix mit Informationen über alle im Spiel vorkommenden Zustands-Aktionspaare aktualisiert werden, sodass die KI gleichzeitig das Verhalten als beginnender und als zweiter Spieler lernt.\n",
    "\n",
    "## Trainingsalgorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(learning_rate, discount_factor, base_exploration_rate, num_episodes=1e4, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}):\n",
    "    \"\"\"\n",
    "    play Tic Tac Toe [num_episodes] times to learn using Q-Learning with the given learning rate and discount_factor.\n",
    "    inputs:\n",
    "        learning_rate - (float) in range [0,1] - alpha\n",
    "        discount_factor - (float) in range [0,1] - gamma\n",
    "        base_exploration_rate - (float) - the starting exploration rate\n",
    "        num_episodes - (int) - number of episodes for training\n",
    "        reward_dict - (dict) - a dictionary specifying the rewards for winning, losing and draw\n",
    "            -> must have keys \"win\", \"loss\", \"draw\"\n",
    "    returns:\n",
    "        (dict) - the Q-table after training with the given parameters\n",
    "    \"\"\"\n",
    "    Q_table = dict()    # assign values to every visited state-action pair\n",
    "    N_table = dict()    # counting how often each state-action pair was visited\n",
    "    action_dict = dict()    # save the possible actions for each state\n",
    "\n",
    "    games = []\n",
    "    exploration_rate = base_exploration_rate\n",
    "    for n in range(num_episodes):\n",
    "        # play episode\n",
    "        state_hist = play_episode(Q_table, action_dict, exploration_rate, discount_factor, learning_rate, reward_dict, N_table)\n",
    "        exploration_rate *= base_exploration_rate\n",
    "        games.append(state_hist)\n",
    "\n",
    "    print(\"final exploration rate:\", exploration_rate)\n",
    "    \n",
    "    return Q_table, N_table, games\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulieren einer Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(Q_table, action_dict, exploration_rate, discount_factor=0.95, learning_rate=0.1, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}, N_table=dict()):\n",
    "    \"\"\"\n",
    "    self-play an entire episode\n",
    "    returns:\n",
    "        (list) - state history\n",
    "        (list) - action history\n",
    "    \n",
    "    action_dict is changed in-place\n",
    "    \"\"\" \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    while True:\n",
    "        state = tuple(field)\n",
    "        state_history.append(state)\n",
    "        # get possible actions\n",
    "        try:\n",
    "            actions = action_dict[state]\n",
    "        except KeyError:\n",
    "            actions = get_actions(field)\n",
    "            action_dict[state] = actions\n",
    "\n",
    "        if len(state_history) > 2: \n",
    "            # we know the state that resulted from the last action\n",
    "            update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict, N_table)\n",
    "\n",
    "        if len(actions) == 0:\n",
    "            break # game has ended\n",
    "\n",
    "        action = choose_Q_action(state, actions, Q_table, exploration_rate=exploration_rate)\n",
    "        action_history.append(action)\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1 # toggle sign between 1 and 2\n",
    "\n",
    "    last_state = state_history[-2]\n",
    "    last_action = action_history[-1]\n",
    "    if not (last_state, last_action) in Q_table.keys():\n",
    "        Q_table[(last_state, last_action)] = 0\n",
    "        N_table[(last_state, last_action)] = 0\n",
    "    # print(\"before\", Q_table[(last_state, last_action)])\n",
    "    Q_table[(last_state, last_action)] += learning_rate*(reward_dict[\"win\"] - Q_table[(last_state, last_action)])\n",
    "    N_table[(last_state, last_action)] += 1\n",
    "    \n",
    "    return state_history\n",
    "\n",
    "\n",
    "def update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict, N_table):\n",
    "    \"\"\"\n",
    "    update the second to last state in the Q-table\n",
    "    returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    prev_state = state_history[-3] # S = state\n",
    "    prev_action = action_history[-2] # A = action\n",
    "    state = state_history[-1] # S' = next state after action A\n",
    "\n",
    "    reward = get_reward(list(state), actions, reward_dict) # R = Reward\n",
    "    next_rewards = [] # Q(S', a') for all actions a'\n",
    "    for action in actions:\n",
    "        try:\n",
    "            next_rewards.append(Q_table[(state, action)])\n",
    "        except KeyError:\n",
    "            Q_table[(state, action)] = 0\n",
    "            N_table[(state, action)] = 0\n",
    "            next_rewards.append(0)\n",
    "\n",
    "    if not (prev_state, prev_action) in Q_table.keys():\n",
    "        Q_table[(prev_state, prev_action)] = 0\n",
    "        N_table[(prev_state, prev_action)] = 0\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        test_value = str(Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "    # Q(S,A) += alpha*(R + gamma * max(S', a') - Q(S,A))\n",
    "    Q_table[(prev_state, prev_action)] += learning_rate*(reward + discount_factor * max(next_rewards, default=0) - Q_table[(prev_state, prev_action)])\n",
    "    N_table[(prev_state, prev_action)] += 1\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        if test_value != str(Q_table[((1,2,0,0,1,2,0,0,0),8)]):\n",
    "            print(\"old value:\", test_value)\n",
    "            print(\"reward was\", reward)\n",
    "            print(\"new value:\", Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "\n",
    "\n",
    "def get_reward(field, actions, reward_dict):\n",
    "    \"\"\"\n",
    "    return the reward for the given field and possible actions\n",
    "    \"\"\"\n",
    "    if len(actions) > 0:\n",
    "        reward = reward_dict[\"move\"]\n",
    "    else:\n",
    "        winner = game_ended(field, get_winner=True)\n",
    "        if winner == 0: #draw\n",
    "            reward = reward_dict[\"draw\"]\n",
    "        else:\n",
    "            reward = reward_dict[\"loss\"]\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswählen einer Aktion\n",
    "Die Aktionen werden nach einer $\\varepsilon$-greedy Strategie ausgewählt. $\\varepsilon$ ist dabei die Wahrscheinlichkeit, dass Exploration, also eine zufällige Aktion gewählt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_Q_action(state, actions, Q_table, exploration_rate=0):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the current Q-table and the current exploration rate\n",
    "    inputs:\n",
    "    -------\n",
    "        state - (tuple) or (list) - the state as a tuple or list\n",
    "        actions (tuple) or (list) - all possible actions in the given state\n",
    "        Q_table - (dict) - dictionary storing all known Q-values\n",
    "        exploration_rate - (float) in [0,1] - probability of choosing exploration rather than exploitation\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # print(\"exploit\", r, exploration_rate)\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            try:\n",
    "                action_values.append(Q_table[(state,action)])\n",
    "            except KeyError:\n",
    "                action_values.append(0)\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwenden des Q-Learning Algorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 0.006726162258635551\nWall time: 1.25 s\n"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "discount_factor = 0.95\n",
    "num_episodes = int(1e4)\n",
    "exploration_rate = 1-(5/num_episodes)\n",
    "reward_dict = {\"win\":1,      # reward for win\n",
    "               \"loss\":-1,    # reward for loss\n",
    "               \"draw\":0,     # reward for draw\n",
    "               \"move\":-0.05} # reward per non-terminal move\n",
    "\n",
    "%time Q_table, N_table, games = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern der Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Q_table(Q_table, filename=\"Q_table.txt\"):\n",
    "    \"\"\"\n",
    "    write the given Q-table into a file\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"Q_table = {\\n\")\n",
    "        for key, value in Q_table.items():\n",
    "            file.write(str(key) + \":\" + str(value) + \",\\n\")\n",
    "        file.write(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_Q_table(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netz\n",
    "### Idee:\n",
    "Wir nutzen die zuvor erzeugt Q-Matrix um ein Neuronales Netzwerk zu trainieren, welches die Q-Funktion approximiert.\n",
    "\n",
    "Dazu wird das Netzwerk ein Zustands-Aktions Paar als Eingabe bekommen und eine Wertung ($\\in \\mathbb{R}$) zurückgeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsdaten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def prepare_data(Q_table):\n",
    "    \"\"\"\n",
    "    prepare the data given in a Q-table for training the neural network\n",
    "    returns:\n",
    "    --------\n",
    "        (np.ndarray) - array of lists of 10 inputs each for the neural network. Every input is in range [0,1]\n",
    "            the first value in each sublist is the action, the other 9 are the state\n",
    "        (np.ndarray) - array of output values. Every output is in range [0,1]\n",
    "    \"\"\"\n",
    "    training_inputs = []\n",
    "    training_outputs = []\n",
    "    for state_action, value in Q_table.items():\n",
    "        # input_data   =      state_info       +    action_info\n",
    "        training_inputs.append( prepare_state_action(*state_action) )\n",
    "        training_outputs.append( (value+1)/3 ) #make sure the target value is in range [0,1]\n",
    "        # training_outputs.append(random.random()) # choose random outputs\n",
    "    return np.array(training_inputs), np.array(training_outputs)\n",
    "\n",
    "def prepare_state_action(state, action):\n",
    "    return [action/8] + [square/2 for square in state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk initialisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei `activation='relu'` und `activation='selu'` scheint das Neuronale Netz zumindest die Aktionen nicht nur nach Index der Aktion zu bewerten. Dennoch lernt das Netz auch damit scheinbar gar nicht.\n",
    "\n",
    "Wir haben auch zahlreichen Kombinationen von `optimizer` und `loss` ausprobiert und kamen immer zum Ergebnis, dass sich der `loss` Wert im Training nach spätestens zwei Epochen nicht mehr wirklich ändert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "def create_ttt_network(hidden_layers):\n",
    "    model = keras.Sequential()\n",
    "    model.add( keras.Input(shape=(10,)) ) # input layer - 10 Nodes\n",
    "    for size in hidden_layers:\n",
    "        model.add( layers.Dense(size, activation='relu') )\n",
    "    model.add( layers.Dense(1) )   # output layer - 1 Node\n",
    "\n",
    "    model.compile(optimizer='Adam', loss='BinaryCrossentropy')\n",
    "    return model\n",
    "\n",
    "# def create_ttt_network(hidden_layers):\n",
    "#     x = keras.Input(shape=(10,))\n",
    "#     inputs = x\n",
    "#     for size in hidden_layers:\n",
    "#         x = layers.Dense(size)(x)\n",
    "#     outputs = layers.Dense(1)(x)   # output layer - 1 Node\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     model.compile(optimizer='RMSprop', loss='Huber')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensorflow.python.keras.losses.BinaryCrossentropy"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "keras.losses.BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<function tensorflow.python.keras.activations.sigmoid(x)>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "keras.activations.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "keras.optimizers.RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk trainieren\n",
    "\n",
    "Hier wird das Netzwerk mit den oben generierten Daten trainiert. Anstatt oben eine neue Q-Matrix zu berechnen kann auch eine bekannte aus einer Datei geladen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, samples, labels, epochs=50, batch_size=32):\n",
    "    return model.fit(samples, labels, epochs=epochs, batch_size=batch_size, use_multiprocessing=True, validation_split=0.1)\n",
    "\n",
    "def import_Q_table(filename=\"Q_table.txt\"):\n",
    "    \"\"\"\n",
    "    import Q_table as dictionary:\n",
    "    keys are state-action pairs as a tuple of a tuple (9 integers: 0/1/2) and an integer (0-8)\n",
    "    values are the corresponding Q-values\n",
    "\n",
    "    Example:\n",
    "        Q_table[((0,0,0,0,1,0,0,0,0),2)] -> 0.3\n",
    "    \"\"\"\n",
    "    Q_table = dict()\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            if line == \"Q_table = {\\n\" or line == \"}\":\n",
    "                continue\n",
    "            state_action, value = line[:-2].split(\":\")\n",
    "            state = tuple([int(x.strip(\" \")) for x in state_action[2:-5].split(\",\")])\n",
    "            action = int(state_action[-2])\n",
    "            Q_table[(state, action)] = float(value)\n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Q-table from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally generate data\n",
    "Q_table = import_Q_table(filename=\"Q_table.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actually train the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben verschiedene Größen des Netzwerks ausprobiert:\n",
    "  - mehrere Hidden Layers 2-5 Ebenen, insgesamt bis zu ca. 100 Neuronen\n",
    "  - keine Hidden Layers\n",
    "  - und vieles dazwischen\n",
    "\n",
    "In allen Fälen haben wir verschieden Werte für `batch_size` (im Breeich 5 bis 50) und `epochs` (im Bereich 1-50) ausprobiert und immer das gleiche Ergebnis festgestellt:\n",
    "Das Neuronale Netz scheint nicht zu lernen. Der `loss` ändert sich ab dem zweiten Durchlauf quasi gar nicht mehr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4811/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4812/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4813/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4814/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4815/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4816/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4817/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4818/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4819/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4820/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4821/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4822/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4823/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4824/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4825/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4826/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4827/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4828/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4829/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4830/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4831/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4832/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4833/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4834/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4835/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4836/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4837/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4838/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4839/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4840/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4841/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4842/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4843/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4844/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4845/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4846/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4847/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4848/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4849/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4850/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4851/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4852/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4853/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4854/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4855/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4856/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4857/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4858/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4859/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4860/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4861/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4862/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4863/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4864/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4865/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4866/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4867/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4868/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4869/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4870/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4871/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4872/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4873/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4874/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4875/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4876/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4877/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4878/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4879/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4880/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4881/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4882/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4883/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4884/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4885/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4886/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4887/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4888/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4889/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4890/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4891/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4892/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4893/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4894/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4895/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4896/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4897/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4898/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4899/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4900/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4901/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4902/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4903/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4904/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4905/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4906/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4907/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4908/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4909/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4910/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4911/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4912/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4913/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4914/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4915/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4916/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4917/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4918/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4919/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4920/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4921/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4922/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4923/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4924/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4925/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4926/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4927/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4928/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4929/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4930/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4931/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4932/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4933/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4934/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4935/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4936/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4937/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4938/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4939/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4940/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4941/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4942/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4943/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4944/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4945/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4946/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4947/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4948/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4949/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4950/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4951/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4952/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4953/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4954/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4955/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4956/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4957/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4958/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4959/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4960/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4961/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4962/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4963/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4964/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4965/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4966/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4967/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4968/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4969/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4970/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4971/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4972/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4973/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4974/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4975/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4976/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4977/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4978/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4979/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4980/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4981/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4982/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4983/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4984/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4985/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4986/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4987/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4988/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4989/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4990/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4991/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4992/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4993/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4994/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4995/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6367\nEpoch 4996/5000\n290/290 [==============================] - 1s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4997/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4998/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 4999/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6368\nEpoch 5000/5000\n290/290 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.6367\n"
    }
   ],
   "source": [
    "# prepare data\n",
    "inputs, outputs = prepare_data(Q_table)\n",
    "# inputs = np.array(list(zip(*inputs)))\n",
    "# inputs = np.array([[i//10 for i in range(1,11)]])\n",
    "# print(inputs)\n",
    "# outputs = np.array([1])\n",
    "print(f\"train network using {inputs.shape} test values\")\n",
    "# create model\n",
    "my_model = create_ttt_network([30,25,20,20])\n",
    "my_model.summary()\n",
    "# train network with given data\n",
    "history = my_model.fit(inputs, outputs, epochs=5000, batch_size=50, use_multiprocessing=True, validation_split=0.1, verbose=1)\n",
    "# history = train_network(my_model, inputs, outputs, epochs=20, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor([[0.3210916]], shape=(1, 1), dtype=float32)\n"
    }
   ],
   "source": [
    "print(my_model(np.array([np.array([0,0,0,0,0,0,0,0,0,0.25])]), training=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktion auswählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_NN_action(state, actions, model, exploration_rate=0):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the given neural network (model) and the current exploration rate\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            action_values.append( model.predict([prepare_state_action(state, action)])[0][0] )\n",
    "        print(list(zip(actions, action_values)))\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spielen: Mensch vs. KI\n",
    "Bevor gespielt werden kann muss dies natürlich mit einigen funktionen vorbereitet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_AI(Q_table, network):\n",
    "    choose_action = get_ai_function(Q_table, network)\n",
    "    start_player = \"\"\n",
    "    while not start_player.lower() in [\"me\", \"ai\"]:\n",
    "        start_player = input(\"Who starts? (me/ ai)\\n\")\n",
    "    \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    print_field(field)\n",
    "\n",
    "    playing = True\n",
    "    while playing:\n",
    "        actions = get_actions(field)\n",
    "        if len(actions) == 0:\n",
    "            break\n",
    "        if start_player == \"ai\":\n",
    "            action = choose_action(tuple(field), actions, exploration_rate=0)\n",
    "        else:\n",
    "            action = get_human_action(actions)\n",
    "            if action == \"end\":\n",
    "                print(\"game interrupted\")\n",
    "                return\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1\n",
    "        print_field(field)\n",
    "        start_player = \"ai\" if start_player == \"me\" else \"me\"\n",
    "        playing = print_winner(field)\n",
    "\n",
    "\n",
    "def print_winner(field):\n",
    "        \"\"\"\n",
    "        if the given field is in a terminal state, print the winner.\n",
    "        \"\"\"\n",
    "        winner = game_ended(field, get_winner=True)\n",
    "        playing = True\n",
    "        if winner != None:\n",
    "            playing = False\n",
    "            if winner == 0:\n",
    "                print(\"draw!\")\n",
    "            elif winner == 1:\n",
    "                print(\"'o' won!\")\n",
    "            else:\n",
    "                print(\"'x' won!\")\n",
    "        return playing\n",
    "\n",
    "\n",
    "def get_human_action(actions):\n",
    "    \"\"\"\n",
    "    get user input for the next action\n",
    "    \"\"\"\n",
    "    action = -5\n",
    "    while not action in actions:\n",
    "        action = input(f\"Choose your action ({str(actions)[1:-1]})\\n\")\n",
    "        try:\n",
    "            action = int(action)\n",
    "            return action\n",
    "        except:\n",
    "            if action.lower() == \"end\":\n",
    "                return \"end\"\n",
    "\n",
    "\n",
    "def get_ai_function(Q_table, network):\n",
    "    \"\"\"\n",
    "    returns a function that chooses an AI-move based on a given field\n",
    "    inputs:\n",
    "        Q_table - (dict) - dictionary with Q_values\n",
    "        network - (keras.model) - keras model\n",
    "    returns:\n",
    "        (function) - a function that chooses an action.\n",
    "            arguments: (state, actions, exploration_rate=0)\n",
    "    \"\"\"\n",
    "    user_input = \"\"\n",
    "    while not user_input.upper() in [\"NN\", \"Q\"]:\n",
    "        user_input = input(\"What AI shall be the opponent?\\n(Neural Network -> NN or Q-Learning -> Q)\\n\")\n",
    "\n",
    "    if user_input.upper() == \"NN\":\n",
    "        def choose_ai_action(state, actions, exploration_rate=0):\n",
    "            return choose_NN_action(state, actions, network, exploration_rate=exploration_rate)\n",
    "    else:\n",
    "        def choose_ai_action(state, actions, exploration_rate=0):\n",
    "            return choose_Q_action(state, actions, Q_table, exploration_rate=exploration_rate)\n",
    "    return choose_ai_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n[(0, 0.3165288), (1, 0.3159535), (2, 0.3156817), (3, 0.31544566), (4, 0.3152082), (5, 0.31495264), (6, 0.31469184), (7, 0.314431), (8, 0.3141702)]\n-------------\n| o |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n-------------\n| o |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   | x |\n-------------\n[(1, 0.3301552), (2, 0.33049333), (3, 0.33030254), (4, 0.3302731), (5, 0.3300371), (6, 0.329782), (7, 0.32949033)]\n-------------\n| o |   | o |\n-------------\n|   |   |   |\n-------------\n|   |   | x |\n-------------\n-------------\n| o |   | o |\n-------------\n|   |   |   |\n-------------\n|   | x | x |\n-------------\n[(1, 0.33474383), (3, 0.33593133), (4, 0.3358034), (5, 0.33553803), (6, 0.33551237)]\n-------------\n| o |   | o |\n-------------\n| o |   |   |\n-------------\n|   | x | x |\n-------------\n-------------\n| o |   | o |\n-------------\n| o | x |   |\n-------------\n|   | x | x |\n-------------\n[(1, 0.33223352), (5, 0.33380777), (6, 0.335029)]\n-------------\n| o |   | o |\n-------------\n| o | x |   |\n-------------\n| o | x | x |\n-------------\n'o' won!\n"
    }
   ],
   "source": [
    "play_AI(Q_table, my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_Q_function(state, Q_table, n=10):\n",
    "    \"\"\"\n",
    "    print the given field with Q-values for the availiable actions (instead of empty squares)\n",
    "    \"\"\"\n",
    "    lines = [\"-\"*(7+3*n)]\n",
    "    for r in range(3):\n",
    "        line = \"|\"\n",
    "        for c, sign in enumerate(state[3*r:3*r+3]):\n",
    "            if sign == 1:\n",
    "                line += \" \"*(n//2) + \"o\" + \" \"*(n//2) + \"|\"\n",
    "            elif sign == 2:\n",
    "                line += \" \"*(n//2) + \"x\" + \" \"*(n//2) + \"|\"\n",
    "            else:\n",
    "                line += f\"{Q_table[(state, 3*r + c)]:{n}.3} |\"\n",
    "        lines.append(line)\n",
    "        lines.append(\"-\"*(7+3*n))\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "\n",
    "def display_NN_function(state, model, n=10):\n",
    "    lines = [\"-\"*(7+3*n)]\n",
    "    for r in range(3):\n",
    "        line = \"|\"\n",
    "        for c, sign in enumerate(state[3*r:3*r+3]):\n",
    "            if sign == 1:\n",
    "                line += \" \"*(n//2) + \"o\" + \" \"*(n//2) + \"|\"\n",
    "            elif sign == 2:\n",
    "                line += \" \"*(n//2) + \"x\" + \" \"*(n//2) + \"|\"\n",
    "            else:\n",
    "                action = model.predict([prepare_state_action(state, 3*r + c)])[0][0]\n",
    "                line += f\"{action:{n}.4} |\"\n",
    "        lines.append(line)\n",
    "        lines.append(\"-\"*(7+3*n))\n",
    "    for line in lines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------------------------------\n|  -0.00255 |     x     |     x     |\n-------------------------------------\n|   -0.0207 |     o     |   -0.0109 |\n-------------------------------------\n|     o     |   -0.0199 |   -0.0105 |\n-------------------------------------\nchosen action: 0\n-------------------------------------\n|    0.3301 |     x     |     x     |\n-------------------------------------\n|    0.3282 |     o     |      0.33 |\n-------------------------------------\n|     o     |    0.3294 |    0.3293 |\n-------------------------------------\n[(0, 0.33009592), (3, 0.32821727), (5, 0.33002704), (7, 0.32936642), (8, 0.32925484)]\nchosen action: 0\n"
    }
   ],
   "source": [
    "field = (1,1,0,0,1,2,2,0,0)\n",
    "field = (0,2,0,0,1,0,1,0,0)\n",
    "field = (0,1,2,0,1,2,0,0,0)\n",
    "field = (0,2,2,0,1,0,1,0,0)\n",
    "display_Q_function(field, Q_table)\n",
    "print(\"chosen action:\", choose_Q_action(field, get_actions(list(field)), Q_table))\n",
    "display_NN_function(field, my_model)\n",
    "print(\"chosen action:\", choose_NN_action(field, get_actions(list(field)), my_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2 has value -0.010000000000000009\n5 has value 0.15451061114415987\n8 has value 0.0011816731690843518\n"
    }
   ],
   "source": [
    "prep_state = [s/2 for s in field]\n",
    "inputs, outputs = prepare_data(Q_table)\n",
    "for state_action, value in zip(inputs, outputs):\n",
    "    if list(state_action[:9]) == prep_state:\n",
    "        print(int(state_action[-1]*8), \"has value\", value*3-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}