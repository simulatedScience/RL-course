{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bit0abb7b7769c2407694c8bddfdec1999b",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2: Q-Learning Tic Tac Toe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir initialisieren Q(s,a) für alle möglichen Zustands- Aktions-Paare mit 0. Anfangs sind also alle möglichen Aktionen gleich wahrscheinlich.\n",
    "\n",
    "Zur Optimierung der Laufzeit berechnen wir nicht in jedem Zustand neu alle möglichen Aktionen, sondern speichern für alle bereits getroffenen Zustände die dort möglichen Aktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktionen zur visuellen Ausgabe von Feldern sowie zum Bestimmen der Aktionen sind, mit kleinen Änderungen, aus der Abgabe zu Blatt 1 kopiert.\n",
    "### grafische Ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen um ein Tik Tac Toe Feld auszugeben\n",
    "def print_field(field):\n",
    "    \"\"\"\n",
    "    print the given field\n",
    "    inputs:\n",
    "        field - (list) or (tuple) - list of 9 ints in (0,1,2)\n",
    "            0 is an empty field\n",
    "    \"\"\"\n",
    "    print_list = convert_field(field)\n",
    "    for i in range(3):\n",
    "        print(\"-\"*13)\n",
    "        print(\"| {} | {} | {} |\".format( *print_list[3*i:3*i+3] ))\n",
    "    print(\"-\"*13)\n",
    "\n",
    "def convert_field(field):\n",
    "    \"\"\"\n",
    "    prepare field for printing by replacing characters\n",
    "    inputs:\n",
    "        field - (list) - list of 9 ints in (0,1,2)\n",
    "            0 is an empty field\n",
    "    returns:\n",
    "        (list) - list of \"o\", \"x\" and \" \" representing the field\n",
    "    \"\"\"\n",
    "    print_list = []\n",
    "    for elem in field:\n",
    "        if elem == 1:\n",
    "            print_list.append(\"o\")\n",
    "        elif elem == 2:\n",
    "            print_list.append(\"x\")\n",
    "        else:\n",
    "            print_list.append(\" \")\n",
    "    return print_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mögliche Aktionen bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen um mögliche Aktionen sowie das Spielende zu bestimmen.\n",
    "from math import sqrt\n",
    "def game_ended(field, get_winner=False):\n",
    "    \"\"\"\n",
    "    checks whether or not a ttt-game has ended\n",
    "    inputs:\n",
    "        field - (list) - list (must not be a tuple) with integer entries 0, 1 and 2\n",
    "    ouputs:\n",
    "        if get_winner is False:\n",
    "            (bool) - whether or not a ttt-game has ended\n",
    "        else:\n",
    "            (int) - sign of the winner. 0 if it is a draw\n",
    "    \"\"\"\n",
    "    n = int(sqrt(len(field)))\n",
    "    rows = [field[n*i:n*i+n] for i in range(n)] #get all rows of the field\n",
    "    columns = [field[i::n] for i in range(n)]   #get all columns of the field\n",
    "    diagonals = [field[::n+1], field[n-1:n**2-2:n-1]] #get both diagonals of the field\n",
    "\n",
    "    win_lists = rows + columns + diagonals\n",
    "    \n",
    "    if not get_winner:\n",
    "        if [1]*n in win_lists or [2]*n in win_lists:\n",
    "            return True\n",
    "        return False\n",
    "    else:\n",
    "        if [1]*n in win_lists:\n",
    "            return 1\n",
    "        if [2]*n in win_lists:\n",
    "            return 2\n",
    "        if not 0 in field:\n",
    "            return 0\n",
    "        \n",
    "\n",
    "def get_actions(field):\n",
    "    \"\"\"\n",
    "    calculate all possible actions for the given field\n",
    "    inputs:\n",
    "        field - (list) - list of 9 ints in (0,1,2)\n",
    "            0 is an empty field\n",
    "    returns:\n",
    "        (list) - list of possible actions as list indices\n",
    "    \"\"\"\n",
    "    actions = []\n",
    "    if 0 in field and not game_ended(field):\n",
    "        for i, elem in enumerate(field):\n",
    "            if elem == 0:\n",
    "                actions.append(i)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementieren der Trainingsumgebung\n",
    "Wir initialisieren ein Dictionary `Q_table`, in dem die bisher probierten Zustands- Aktions- Paare mit einer Wertung gespeichert werden.\n",
    "\n",
    "Schlüssel sind dazu Tupel (Zustand, Aktion), wobei die Aktion eine Zahl von 0 bis 8 und Zustand ein Tupel mit 9 Elementen aus {0,1,2} sind.\n",
    "\n",
    "Außerdem initialisieren wir `action_dict` um alle in einem Zustand (als Schlüssel gegeben) möglichen Aktionen zu speichern.\n",
    "\n",
    "Die `eploration_rate` in der $n$-ten Episode ist $\\varepsilon^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(learning_rate, discount_factor, base_exploration_rate, num_episodes=1e4, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}):\n",
    "    \"\"\"\n",
    "    play Tic Tac Toe [num_episodes] times to learn using Q-Learning with the given learning rate and discount_factor.\n",
    "    inputs:\n",
    "        learning_rate - (float) in range [0,1] - alpha\n",
    "        discount_factor - (float) in range [0,1] - gamma\n",
    "        base_exploration_rate - (float) - the starting exploration rate\n",
    "        num_episodes - (int) - number of episodes for training\n",
    "        reward_dict - (dict) - a dictionary specifying the rewards for winning, losing and draw\n",
    "            -> must have keys \"win\", \"loss\", \"draw\"\n",
    "    returns:\n",
    "        (dict) - the Q-table after training with the given parameters\n",
    "    \"\"\"\n",
    "    Q_table = dict()\n",
    "    action_dict = dict()\n",
    "\n",
    "    games = []\n",
    "    exploration_rate = base_exploration_rate\n",
    "    for n in range(num_episodes):\n",
    "        # play episode\n",
    "        state_hist = play_episode(Q_table, action_dict, exploration_rate, discount_factor, learning_rate, reward_dict)\n",
    "        exploration_rate *= base_exploration_rate\n",
    "        games.append(state_hist)\n",
    "\n",
    "    print(\"final exploration rate:\", exploration_rate)\n",
    "    \n",
    "    return Q_table, games\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spielen einer Episode\n",
    "\n",
    "Während dem Training spielt der Algorithmus gegen sich selbst und erforscht so gleichzeitig die Zustände, wenn er selbst anfängt und wenn der Gegner anfängt.\n",
    "\n",
    "Durch festlegen des Zeichens des Startspielers ist in jedem Zustand eindeutig, welches Zeichen als nächstes gesetzt werden muss.\n",
    "\n",
    "Zur späteren Aktualisierung der Q-Werte werden alle Zustände und gewählten Aktionen gespeichert (in den Listen `state_history` und `action_history`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(Q_table, action_dict, exploration_rate, discount_factor=0.95, learning_rate=0.1, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0, \"move\":-0.05}):\n",
    "    \"\"\"\n",
    "    self-play an entire episode\n",
    "    returns:\n",
    "        (list) - state history\n",
    "        (list) - action history\n",
    "    \n",
    "    action_dict is changed in-place\n",
    "    \"\"\" \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    while True:\n",
    "        state = tuple(field)\n",
    "        state_history.append(state)\n",
    "        # get possible actions\n",
    "        try:\n",
    "            actions = action_dict[state]\n",
    "        except KeyError:\n",
    "            actions = get_actions(field)\n",
    "            action_dict[state] = actions\n",
    "\n",
    "        if len(state_history) > 2: \n",
    "            # we know the state that resulted from the last action\n",
    "            update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict)\n",
    "\n",
    "        if len(actions) == 0:\n",
    "            break # game has ended\n",
    "\n",
    "        action = choose_action(state, actions, Q_table, exploration_rate=exploration_rate)\n",
    "        action_history.append(action)\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1 # toggle sign between 1 and 2\n",
    "\n",
    "    last_state = state_history[-2]\n",
    "    last_action = action_history[-1]\n",
    "    if not (last_state, last_action) in Q_table.keys():\n",
    "        Q_table[(last_state, last_action)] = 0\n",
    "    # print(\"before\", Q_table[(last_state, last_action)])\n",
    "    Q_table[(last_state, last_action)] += learning_rate*(reward_dict[\"win\"] - Q_table[(last_state, last_action)])\n",
    "    # print(\"after\", Q_table[(last_state, last_action)])\n",
    "    \n",
    "    return state_history\n",
    "\n",
    "\n",
    "def update_q_table(Q_table, state_history, action_history, actions, discount_factor, learning_rate, reward_dict):\n",
    "    \"\"\"\n",
    "    update the second to last state in the Q-table\n",
    "    returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    prev_state = state_history[-3] # S = state\n",
    "    prev_action = action_history[-2] # A = action\n",
    "    state = state_history[-1] # S' = next state after action A\n",
    "\n",
    "    reward = get_reward(list(state), actions, reward_dict) # R = Reward\n",
    "    next_rewards = [] # Q(S', a') for all actions a'\n",
    "    for action in actions:\n",
    "        try:\n",
    "            next_rewards.append(Q_table[(state, action)])\n",
    "        except KeyError:\n",
    "            Q_table[(state, action)] = 0\n",
    "            next_rewards.append(0)\n",
    "\n",
    "    if not (prev_state, prev_action) in Q_table.keys():\n",
    "        Q_table[(prev_state, prev_action)] = 0\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        test_value = str(Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "    Q_table[(prev_state, prev_action)] += learning_rate*(reward + discount_factor * max(next_rewards, default=0) - Q_table[(prev_state, prev_action)])\n",
    "    if ((1,2,0,0,1,2,0,0,0),8) in Q_table.keys():\n",
    "        if test_value != str(Q_table[((1,2,0,0,1,2,0,0,0),8)]):\n",
    "            print(\"old value:\", test_value)\n",
    "            print(\"reward was\", reward)\n",
    "            print(\"new value:\", Q_table[((1,2,0,0,1,2,0,0,0),8)])\n",
    "\n",
    "\n",
    "def get_reward(field, actions, reward_dict):\n",
    "    \"\"\"\n",
    "    return the reward for the given field and possible actions\n",
    "    \"\"\"\n",
    "    if len(actions) > 0:\n",
    "        reward = reward_dict[\"move\"]\n",
    "    else:\n",
    "        winner = game_ended(field, get_winner=True)\n",
    "        if winner == 0: #draw\n",
    "            reward = reward_dict[\"draw\"]\n",
    "        else:\n",
    "            reward = reward_dict[\"loss\"]\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wählen einer Aktion\n",
    "Zum Wählen der Aktion wird eine $\\varepsilon$-greedy Strategie genutzt.\n",
    "\n",
    "Wird damit ausgewählt bekanntes Wissen zu nutzen (Exploitation), so wird, falls es mehrere Aktionen mit maximalem Wert gibt, eine zufällige davon ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_action(state, actions, Q_table, exploration_rate=0):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the current Q-table and the current exploration rate\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # print(\"exploit\", r, exploration_rate)\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            try:\n",
    "                action_values.append(Q_table[(state,action)])\n",
    "            except KeyError:\n",
    "                action_values.append(0)\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwenden des Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit all diesen Funktionen können wir nun endlich den Algorithmus testen. Es epfiehlt sich den Agent mindestens über 10.000 Spiele zu trainieren, dies sollte nur wenige Sekunden dauern. Die Trainingszeit steigt wie zu erwarten ziemlich linear mit der Anzahl an Trainingsspielen.\n",
    "\n",
    "Im Folgenden wird die exploration_rate mit `1-(5/num_episodes)` so gewählt, dass sie am Ende des Trainings nahe 0 kommt, aber nicht zu lange so klein ist.\n",
    "\n",
    "Jeder nicht-terminal Zug wird mit einem kleinen Wert bestraft um schnelles gewinnen zu fördern.\n",
    "\n",
    "Es ist schwierig eine optimale Lernrate $\\alpha$ oder einen besonders guten discount_factor $\\gamma$ zu finden, da das Evaluieren, wie gut die KI nach dem Training noch nicht automatisiert ist.\n",
    "\n",
    "Mit mehr Zeit wäre es denkbar genetische Algorithmen für diese Werte anzuwenden und die trainierten Agenten gegeneinander spielen zu lassen. So könnte man bessere Startwerte finen.\n",
    "Dabei würden relativ kurze Trainingszeiten verwendet werden um dann mit den gefundenen, optimierten Startwerten hoffentlich einen besseren Agenten trainieren zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 0.00673676792504109\nWall time: 12 s\n16161\n"
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "discount_factor = 0.95\n",
    "num_episodes = int(1e5)\n",
    "exploration_rate = 1-(5/num_episodes)\n",
    "reward_dict = {\"win\":2,      # reward for win\n",
    "               \"loss\":-1,    # reward for loss\n",
    "               \"draw\":0,     # reward for draw\n",
    "               \"move\":-0.05} # reward per non-terminal move\n",
    "\n",
    "%time Q_table, games = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)\n",
    "print(len(Q_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns einzelne Spiele aus dem Trainingsprozess ansehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n-------------\n|   |   |   |\n-------------\n|   | o |   |\n-------------\n|   |   |   |\n-------------\n-------------\n|   |   |   |\n-------------\n|   | o |   |\n-------------\n| x |   |   |\n-------------\n-------------\n|   | o |   |\n-------------\n|   | o |   |\n-------------\n| x |   |   |\n-------------\n-------------\n|   | o |   |\n-------------\n|   | o |   |\n-------------\n| x | x |   |\n-------------\n-------------\n|   | o |   |\n-------------\n|   | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o |   |\n-------------\n|   | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o |   |\n-------------\n| o | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o | x |\n-------------\n| o | o |   |\n-------------\n| x | x | o |\n-------------\n-------------\n| x | o | x |\n-------------\n| o | o | o |\n-------------\n| x | x | o |\n-------------\n#########################\n"
    }
   ],
   "source": [
    "def show_game(games, i):\n",
    "    for state in games[i]:\n",
    "        print_field(state)\n",
    "        # print(game_ended(list(state)))\n",
    "    print(\"#\"*25)\n",
    "# show_game(games, 0)\n",
    "show_game(games, num_episodes-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testen durch Menschen\n",
    "\n",
    "Mit der folgenden Funktion können wir auch die KI testen indem wir selbst gegen sie spielen. Dabei werden die Züge ausschließlich nach exploitation gewählt, also immer ein optimaler Zug entsprechend den Q-Werten aus der tabelle `Q_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\n-------------\n| o |   |   |\n-------------\n|   |   |   |\n-------------\n|   |   |   |\n-------------\naction had value: -0.061494505791876916\n-------------\n| o |   |   |\n-------------\n|   |   |   |\n-------------\n|   | x |   |\n-------------\n-------------\n| o |   |   |\n-------------\n|   | o |   |\n-------------\n|   | x |   |\n-------------\naction had value: -0.057117543801702975\n-------------\n| o |   |   |\n-------------\n|   | o |   |\n-------------\n|   | x | x |\n-------------\n-------------\n| o |   |   |\n-------------\n|   | o |   |\n-------------\n| o | x | x |\n-------------\naction had value: -0.17817726973226883\n-------------\n| o |   | x |\n-------------\n|   | o |   |\n-------------\n| o | x | x |\n-------------\n-------------\n| o |   | x |\n-------------\n| o | o |   |\n-------------\n| o | x | x |\n-------------\n'o' won!\n"
    }
   ],
   "source": [
    "def play_AI(Q_table):\n",
    "    start_player = \"\"\n",
    "    while not start_player in [\"me\", \"ai\"]:\n",
    "        start_player = input(\"Who starts? (me/ ai)\\n\")\n",
    "    \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    print_field(field)\n",
    "\n",
    "    playing = True\n",
    "    while playing:\n",
    "        actions = get_actions(field)\n",
    "        if start_player == \"ai\":\n",
    "            action = choose_action(tuple(field), actions, Q_table, exploration_rate=0)\n",
    "            print(\"action had value:\", Q_table[(tuple(field), action)])\n",
    "        else:\n",
    "            action = -5\n",
    "            while not action in actions:\n",
    "                action = input(f\"Choose your action ({str(actions)[1:-1]})\\n\")\n",
    "                if action == \"end\":\n",
    "                    playing = False\n",
    "                    break\n",
    "                try:\n",
    "                    action = int(action)\n",
    "                except:\n",
    "                    pass\n",
    "        if not playing:\n",
    "            print(\"game ended\")\n",
    "            break\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1\n",
    "        print_field(field)\n",
    "        start_player = \"ai\" if start_player == \"me\" else \"me\"\n",
    "\n",
    "        winner = game_ended(field,get_winner=True)\n",
    "        if winner != None:\n",
    "            playing = False\n",
    "            if winner == 0:\n",
    "                print(\"draw!\")\n",
    "            elif winner == 1:\n",
    "                print(\"'o' won!\")\n",
    "            else:\n",
    "                print(\"'x' won!\")\n",
    "\n",
    "play_AI(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selbst nach 1.000.000 Trainingsspielen kann man, mit etwas Glück, noch gegen den Algorithmus gewinnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n| o | x |   |\n-------------\n| o | o |   |\n-------------\n| x | o | x |\n-------------\naction 2 has value 0.0\naction 5 has value 0.0\n"
    }
   ],
   "source": [
    "field = (1,2,0,1,1,0,2,1,2)\n",
    "print_field(field)\n",
    "for action in get_actions(list(field)):\n",
    "    print(f\"action {action} has value {Q_table[(field,action)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste zwei unterschiedliche KIs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ais(Q_table_1, Q_table_2, n_games=100, exploration_rate=0.1):\n",
    "    results = [0,0,0] # AI-1 victories, AI-2 victories, draws\n",
    "    for n in range(n_games):\n",
    "        field = [0]*9\n",
    "        active_index = n%2\n",
    "        Q_tables = (Q_table_1, Q_table_2)\n",
    "        sign = 1\n",
    "        winner = None\n",
    "        while winner==None:\n",
    "            actions = get_actions(field)\n",
    "            if actions == []:\n",
    "                print_field(field)\n",
    "            action = choose_action(tuple(field), actions, Q_tables[active_index], exploration_rate=exploration_rate)\n",
    "            field[action] = sign\n",
    "            sign = sign%2 + 1\n",
    "            winner = game_ended(field, get_winner=True)\n",
    "        if winner == 0:\n",
    "            results[2] += 1\n",
    "        elif n%2 == 0:\n",
    "            if winner == 1:\n",
    "                results[0] += 1\n",
    "            else:\n",
    "                results[1] += 1\n",
    "        elif n%2 == 1:\n",
    "            if winner == 1:\n",
    "                results[1] += 1\n",
    "            else:\n",
    "                results[0] += 1\n",
    "\n",
    "    print(\"KI 1 hat {} Mal gewonnen.\\nKI 2 hat {} Mal gewonnen.\\nEs gab {} Unentschieden.\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train AI 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 0.0056245027593172965\nWall time: 32.8 ms\n"
    }
   ],
   "source": [
    "# KI 1\n",
    "learning_rate = 0.05\n",
    "discount_factor = 0.80\n",
    "num_episodes = int(1e2)\n",
    "exploration_rate = 1-(5/num_episodes)\n",
    "reward_dict = {\"win\":1,      # reward for win\n",
    "               \"loss\":-2,    # reward for loss\n",
    "               \"draw\":0.5,     # reward for draw\n",
    "               \"move\":0} # reward per non-terminal move\n",
    "\n",
    "%time Q_table_1, _ = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train AI 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "final exploration rate: 0.04978333441067581\nWall time: 12.2 s\n"
    }
   ],
   "source": [
    "# KI 2\n",
    "\n",
    "learning_rate = 0.05\n",
    "discount_factor = 0.75\n",
    "num_episodes = int(1e5)\n",
    "exploration_rate = 1-(3/num_episodes)\n",
    "reward_dict = {\"win\":1,      # reward for win\n",
    "               \"loss\":-1,    # reward for loss\n",
    "               \"draw\":0,     # reward for draw\n",
    "               \"move\":-0.05} # reward per non-terminal move\n",
    "\n",
    "%time Q_table_2, _ = train_q_learning(learning_rate, discount_factor, exploration_rate, num_episodes=num_episodes, reward_dict=reward_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play AI 1 vs. AI 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "KI 1 hat 3731 Mal gewonnen.\nKI 2 hat 2777 Mal gewonnen.\nEs gab 3492 Unentschieden.\n"
    }
   ],
   "source": [
    "test_ais(Q_table_1, Q_table_2, n_games=10_000, exploration_rate=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch nach Ausprobieren zahlreicher Werte für $\\alpha$, $\\gamma$, $\\varepsilon$ und für verschiedene Rewards konnte ich keine Kombination finden, die die hier zweite KI konstant schlägt.\n",
    "\n",
    "Interessanterweise führt aber eine geringere Anzahl an Trainingsspielen zu mehr Siegen. Das zeigt deutlich, dass die KIs auch nach längerem Selbsttraining nicht perfekt sind. Eine genaue Erklärung für dieses Verhalten habe ich aber nicht gefunden.\n",
    "\n",
    "Die `exploration_rate` in den Testspielen hat kaum Einfluss auf das Sieg-Verhältnis, nur auf die gesamte Anzahl an Unentschieden."
   ]
  }
 ]
}