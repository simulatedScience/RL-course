{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bit0abb7b7769c2407694c8bddfdec1999b",
   "display_name": "Python 3.7.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2: Q-Learning Tic Tac Toe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir initialisieren Q(s,a) für alle möglichen Zustands- Aktions-Paare mit 0. Anfangs sind also alle möglichen Aktionen gleich wahrscheinlich.\n",
    "\n",
    "Zur Optimierung der Laufzeit berechnen wir nicht in jedem Zustand neu alle möglichen Aktionen, sondern speichern für alle bereits getroffenen Zustände die dort möglichen Aktionen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktionen zur visuellen Ausgabe von Feldern sowie zum Bestimmen der Aktionen sind, mit kleinen Änderungen, aus der Abgabe zu Blatt 1 kopiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen um ein Tik Tac Toe Feld auszugeben\n",
    "def print_field(field):\n",
    "    \"\"\"\n",
    "    print the given field\n",
    "    inputs:\n",
    "        field - (list) or (tuple) - list of 9 ints in (0,1,2)\n",
    "            0 is an empty field\n",
    "    \"\"\"\n",
    "    print_list = convert_field(field)\n",
    "    for i in range(3):\n",
    "        print(\"-\"*13)\n",
    "        print(\"| {} | {} | {} |\".format( *print_list[3*i:3*i+3] ))\n",
    "    print(\"-\"*13)\n",
    "\n",
    "def convert_field(field):\n",
    "    \"\"\"\n",
    "    prepare field for printing by replacing characters\n",
    "    inputs:\n",
    "        field - (list) - list of 9 ints in (0,1,2)\n",
    "            0 is an empty field\n",
    "    returns:\n",
    "        (list) - list of \"o\", \"x\" and \" \" representing the field\n",
    "    \"\"\"\n",
    "    print_list = []\n",
    "    for elem in field:\n",
    "        if elem == 1:\n",
    "            print_list.append(\"o\")\n",
    "        elif elem == 2:\n",
    "            print_list.append(\"x\")\n",
    "        else:\n",
    "            print_list.append(\" \")\n",
    "    return print_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen um mögliche Aktionen sowie das Spielende zu bestimmen.\n",
    "from math import sqrt\n",
    "def game_ended(field, get_winner=False):\n",
    "    \"\"\"\n",
    "    checks whether or not a ttt-game has ended\n",
    "    inputs:\n",
    "        field - (list) - list (must not be a tuple) with integer entries 0, 1 and 2\n",
    "    ouputs:\n",
    "        if get_winner is False:\n",
    "            (bool) - whether or not a ttt-game has ended\n",
    "        else:\n",
    "            (int) - sign of the winner. 0 if it is a draw\n",
    "    \"\"\"\n",
    "    n = int(sqrt(len(field)))\n",
    "    rows = [field[n*i:n*i+n] for i in range(n)] #get all rows of the field\n",
    "    columns = [field[i::n] for i in range(n)]   #get all columns of the field\n",
    "    diagonals = [field[::n+1], field[n-1:n**2-2:n-1]] #get both diagonals of the field\n",
    "\n",
    "    win_lists = rows + columns + diagonals\n",
    "    \n",
    "    if not get_winner:\n",
    "        if [1]*n in win_lists or [2]*n in win_lists:\n",
    "            return True\n",
    "        return False\n",
    "    else:\n",
    "        if not 0 in field:\n",
    "            return 0\n",
    "        if [1]*n in win_lists:\n",
    "            return 1\n",
    "        if [2]*n in win_lists:\n",
    "            return 2\n",
    "        \n",
    "\n",
    "def get_actions(field):\n",
    "    \"\"\"\n",
    "    calculate all possible actions for the given field\n",
    "    inputs:\n",
    "        field - (list) - list of 9 ints in (0,1,2)\n",
    "            0 is an empty field\n",
    "    returns:\n",
    "        (list) - list of possible actions as list indices\n",
    "    \"\"\"\n",
    "    actions = []\n",
    "    if 0 in field and not game_ended(field):\n",
    "        for i, elem in enumerate(field):\n",
    "            if elem == 0:\n",
    "                actions.append(i)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir initialisieren ein Dictionary `Q_table`, in dem die bisher probierten Zustands- Aktions- Paare mit einer Wertung gespeichert werden.\n",
    "\n",
    "Schlüssel sind dazu Tupel (Zustand, Aktion), wobei die Aktion eine Zahl von 0 bis 8 und Zustand ein Tupel mit 9 Elementen aus {0,1,2} sind.\n",
    "\n",
    "Außerdem initialisieren wir `action_dict` um alle in einem Zustand (als Schlüssel gegeben) möglichen Aktionen zu speichern.\n",
    "\n",
    "_**mögliche Optimierung:**\n",
    "Speichere zusätzlich den Folgezustand zu jeder Aktion._\n",
    "\n",
    "Da Reward nur am Ende eines Spiels vergeben wird, können die Werte für Q(s,a) auch nur am Ende einer Episode aktualisiert werden und nicht während der Episode.\n",
    "\n",
    "Erst dann kann aus den vergangenen Zuständen und gewählten Aktionen ein neuer Wert für die entsprechenden Paare berechnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Reward in nicht-Teriminal Zuständen ist immer 0. Tic Tac Toe ist ausreichend einfach, dass diese Festlegung nicht zu Problemen beim Training führen sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(learning_rate, discount_factor, num_episodes=1e4, reward_dict={\"win\":1, \"loss\":-1, \"draw\":0}):\n",
    "    \"\"\"\n",
    "    play Tic Tac Toe [num_episodes] times to learn using Q-Learning with the given learning rate and discount_factor.\n",
    "    inputs:\n",
    "        learning_rate - (float) in range [0,1] - alpha\n",
    "        discount_factor - (float) in range [0,1] - gamma\n",
    "        num_episodes - (int) - number of episodes for training\n",
    "        reward_dict - (dict) - a dictionary specifying the rewards for winning, losing and draw\n",
    "            -> must have keys \"win\", \"loss\", \"draw\"\n",
    "    returns:\n",
    "        (dict) - the Q-table after training with the given parameters\n",
    "    \"\"\"\n",
    "    Q_table = dict()\n",
    "    action_dict = dict()\n",
    "    exploration_rate = 1\n",
    "\n",
    "    for n in range(num_episodes):\n",
    "        # play episode\n",
    "        states, actions = play_episode(Q_table, action_dict, exploration_rate)\n",
    "        # evaluate results\n",
    "        winner = game_ended(list(states[-1]), get_winner=True)\n",
    "        states.reversed()\n",
    "        actions.reversed()\n",
    "        # pair with rewards for player 2 and 1 (in that order)\n",
    "        if winner == 0:\n",
    "            reward = (reward_dict[\"draw\"], reward_dict[\"draw\"])\n",
    "        else:\n",
    "            reward = (reward_dict[\"win\"], reward_dict[\"loss\"])\n",
    "\n",
    "        for i, state, action in zip([i for i in range(9)], states, actions):\n",
    "            if i == 0: # terminal State\n",
    "                Q_table[(state, action)] += learning_rate * reward[0] #-Q_table[(state, action)] # ???\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_rewards = [Q_table(next_state, action) for action in action_dict(next_state)] # TODO\n",
    "                Q_table[(state, action)] += learning_rate * (max(next_rewards) - Q_table[(state, action)])\n",
    "                next_state = state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während dem Training spielt der Algorithmus gegen sich selbst und erforscht so gleichzeitig die Zustände, wenn er selbst anfängt und wenn der Gegner anfängt.\n",
    "\n",
    "Durch festlegen des Zeichens des Startspielers ist in jedem Zustand eindeutig, welches Zeichen als nächstes gesetzt werden muss.\n",
    "\n",
    "Zur späteren Aktualisierung der Q-Werte werden alle Zustände und gewählten Aktionen gespeichert (in den Listen `state_history` und `action_history`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(Q_table, action_dict, exploration_rate):\n",
    "    \"\"\"\n",
    "    self-play an entire episode\n",
    "    returns:\n",
    "        (list) - state history\n",
    "        (list) - action history\n",
    "    \n",
    "    action_dict is changed in-place\n",
    "    \"\"\" \n",
    "    field = [0 for _ in range(9)]\n",
    "    sign = 1\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    while True:\n",
    "        state = tuple(field)\n",
    "        # get possible actions\n",
    "        try:\n",
    "            actions = action_dict[state]\n",
    "        except KeyError:\n",
    "            actions = get_actions(field)\n",
    "            action_dict[state] = actions\n",
    "        if actions == []:\n",
    "            break # game has ended\n",
    "\n",
    "        action = choose_action(state, actions, Q_table, exploration_rate)\n",
    "        field[action] = sign\n",
    "        sign = sign%2 + 1 # toggle sign between 1 and 2\n",
    "\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "    \n",
    "    return state_history, action_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Wählen der Aktion wird eine $\\varepsilon$-greedy Strategie genutzt.\n",
    "\n",
    "Wird damit ausgewählt bekanntes Wissen zu nutzen, so wird eine zufällige Aktion mit maximalem Wert ausgewählt, falls es mehrere Aktionen mit maximalem Wert gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def choose_action(state, actions, Q_table, exploration_rate):\n",
    "    \"\"\"\n",
    "    choose an action based on the possible actions, the current Q-table and the current exploration rate\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    if r > exploration_rate:\n",
    "        # exploit knowledge\n",
    "        action_values = []\n",
    "        for action in actions:\n",
    "            try:\n",
    "                action_values.append(Q_table[(state,action)])\n",
    "            except KeyError:\n",
    "                action_values.append(0)\n",
    "        max_value = max(action_values)\n",
    "        best_actions = []\n",
    "        for action, value in zip(actions, action_values):\n",
    "            if value == max_value:\n",
    "                best_actions.append(action)\n",
    "        # return random action with maximum expected reward\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    # explore environment through random move\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------\n| o | o | x |\n-------------\n| x |   | o |\n-------------\n|   | x | o |\n-------------\n[[1, 0, 1], [2, 0, 0]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "field = [1,1,2,2,0,1,0,2,1]\n",
    "print_field(field)\n",
    "actions = get_actions(field)\n",
    "choose_action(tuple(field), actions, dict(), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}